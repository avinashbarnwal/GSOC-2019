\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}

\title{Medium1}
\author{Avinash Barnwal}
\date{March 2019}

\begin{document}

\maketitle

\begin{flushleft}

\textbf{Problem} Write a vignette in LaTeX or MathJax explaining how to use the logistic loss with non-uniform weights to get the binomial loss function in xgboost.

\textbf{Answer -} 
Following is likelihood function of binomial loss with count of trials and success :-

\begin{equation}
    L(\theta;x) = \prod_{i=1}^n {n_i\choose n_iy_i}p(\theta;x)^{(n_iy_i)}(1-p(\theta;x))^{(n_i-n_iy_i)}
\end{equation}


\begin{equation}
    log(L(\theta;x)) = \sum_{i=1}^n n_iy log(p(\theta;x)) + {(n_i-n_iy_i)}log(1-p(\theta;x)) + \sum_{i=1}^n log({n_i\choose n_iy_i})
\end{equation}


\begin{equation}
    \hat{\theta} = \underset{\theta}{\operatorname{arg\,max}}\sum_{i=1}^n n_iy_i log(p(\theta;x)) + {(n_i-n_iy_i)}log(1-p(\theta;x)) + \sum_{i=1}^n log({n_i\choose n_iy_i})
\end{equation}

\begin{equation}
    \hat{\theta} = \underset{\theta}{\operatorname{arg\,max}}\sum_{i=1}^n n_i\{y_i log(p(\theta;x)) + {(1-y_i)}log(1-p(\theta;x))\} + \sum_{i=1}^n log({n_i\choose n_iy_i})
\end{equation}

Above equation is nothing but logistic loss with weight $n_i$ and $y_i$ is proportions of success rather than 0-1 like in logistic loss.

Therefore , for xgboost $y_i$(label in design matrix)  will change to proportion of success and weight will be passed with values equal to number of trials with logistic loss.

\end{flushleft}


\end{document}
