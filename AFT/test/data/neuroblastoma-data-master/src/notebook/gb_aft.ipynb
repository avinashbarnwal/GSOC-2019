{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                                   as np\n",
    "import pandas                                  as pd\n",
    "import matplotlib.pyplot                       as plt\n",
    "import math\n",
    "import random\n",
    "from   sklearn                                 import ensemble\n",
    "from   sklearn                                 import datasets\n",
    "from   sklearn.utils                           import shuffle\n",
    "from   sklearn.metrics                         import mean_squared_error\n",
    "from   sklearn.datasets                        import load_boston\n",
    "from   sklearn.model_selection                 import cross_val_score\n",
    "from   sklearn.tree                            import DecisionTreeRegressor\n",
    "from   sklearn.model_selection                 import train_test_split\n",
    "from   sklearn.ensemble._gradient_boosting     import predict_stages\n",
    "from   sklearn.ensemble._gradient_boosting     import predict_stage\n",
    "from   abc                                     import abstractmethod\n",
    "from   scipy.special                           import expit\n",
    "from   sklearn.utils                           import check_array\n",
    "from   sklearn.tree._tree                      import DTYPE\n",
    "from   sklearn.tree._tree                      import TREE_LEAF\n",
    "from   scipy.special                           import logsumexp\n",
    "from   sklearn.utils                           import check_random_state\n",
    "from   sklearn.ensemble.gradient_boosting      import ZeroEstimator\n",
    "from   _aft_loss                               import loss, negative_gradient,hessian\n",
    "import sys\n",
    "import graphviz\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.externals.six import StringIO  \n",
    "from   IPython.display import Image  \n",
    "from   sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "    \n",
    "    \"\"\"Abstract base class for various loss functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_classes : int\n",
    "        Number of classes\n",
    "    Attributes\n",
    "    ----------\n",
    "    K : int\n",
    "        The number of regression trees to be induced;\n",
    "        1 for regression and binary classification;\n",
    "        ``n_classes`` for multi-class classification.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    is_multi_class = False\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        self.K = n_classes\n",
    "\n",
    "    def init_estimator(self):\n",
    "        \n",
    "        \"\"\"Default ``init`` estimator for loss function. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, y_lower, y_higher,pred,dist,sigma,metrics,sample_weight=None):\n",
    "        \n",
    "        \"\"\"Compute the loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples,)\n",
    "            True labels\n",
    "        pred : array, shape (n_samples,)\n",
    "            Predicted labels\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            Sample weights.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def negative_gradient(self, y_lower, y_higher, pred,dist,sigma, **kargs):\n",
    "        \n",
    "        \"\"\"Compute the negative gradient.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples,)\n",
    "            The target labels.\n",
    "        pred : array, shape (n_samples,)\n",
    "            The predictions.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_terminal_regions(self, tree, X, y_lower,y_higher, residual, y_pred, dist, sigma, sample_weight, sample_mask, learning_rate=1.0):\n",
    "        \n",
    "        \"\"\"Update the terminal regions (=leaves) of the given tree and\n",
    "        updates the current predictions of the model. Traverses tree\n",
    "        and invokes template method '_update_terminal_region'.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : tree.Tree\n",
    "            The tree object.\n",
    "        X : array, shape (n, m)\n",
    "            The data array.\n",
    "        y : array, shape (n,)\n",
    "            The target labels.\n",
    "        residual : array, shape (n,)\n",
    "            The residuals (usually the negative gradient).\n",
    "        y_pred : array, shape (n,)\n",
    "            The predictions.\n",
    "        sample_weight : array, shape (n,)\n",
    "            The weight of each sample.\n",
    "        sample_mask : array, shape (n,)\n",
    "            The sample mask to be used.\n",
    "        learning_rate : float, default=0.1\n",
    "            learning rate shrinks the contribution of each tree by\n",
    "             ``learning_rate``.\n",
    "        k : int, default 0\n",
    "            The index of the estimator being updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute leaf for each sample in ''X''.\n",
    "        \n",
    "        terminal_regions                      = tree.apply(X)\n",
    "\n",
    "        # mask all which are not in sample mask.\n",
    "        masked_terminal_regions               = terminal_regions.copy()\n",
    "        masked_terminal_regions[~sample_mask] = -1\n",
    "\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            \n",
    "            self._update_terminal_region(tree, masked_terminal_regions,\n",
    "                                         leaf, X, y_lower, y_higher, residual, y_pred,dist,sigma, sample_weight)\n",
    "        \n",
    "        y_pred = y_pred + (learning_rate* tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
    "        return y_pred\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y_lower,y_higher,\n",
    "                                residual,pred,dist,sigma, sample_weight):\n",
    "        \n",
    "        \"\"\"Template method for updating terminal regions (=leaves).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroEstimator:\n",
    "    \n",
    "    \"\"\"An estimator that simply predicts zero.\n",
    "    .. deprecated:: 0.21\n",
    "        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version\n",
    "        0.21 and will be removed in version 0.23.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y_lower,y_higher,X_val, y_lower_val,y_higher_val, sample_weight=None):\n",
    "        \n",
    "        \"\"\"Fit the estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : numpy, shape (n_samples, n_targets)\n",
    "            Target values. Will be cast to X's dtype if necessary\n",
    "        sample_weight : array, shape (n_samples,)\n",
    "            Individual weights for each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.issubdtype(y_lower.dtype, np.signedinteger):\n",
    "            # classification\n",
    "            self.n_classes = np.unique(y_lower).shape[0]\n",
    "            if self.n_classes == 2:\n",
    "                self.n_classes = 1\n",
    "        else:\n",
    "            # regression\n",
    "            self.n_classes = 1\n",
    "\n",
    "    def predict(self, X,X_val):\n",
    "        \"\"\"Predict labels\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        #check_is_fitted(self, 'n_classes')\n",
    "\n",
    "        y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)\n",
    "        y.fill(0.0)\n",
    "        \n",
    "        y_val = np.empty((X_val.shape[0], self.n_classes), dtype=np.float64)\n",
    "        y_val.fill(0.0)\n",
    "        return y,y_val\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFT(LossFunction):\n",
    "    \"\"\"Cox Partial Likelihood\"\"\"\n",
    "\n",
    "    def __call__(self, y_lower, y_higher, y_pred, dist, sigma, metrics, sample_weight=None):\n",
    "        \"\"\"Compute the partial likelihood of prediction ``y_pred`` and ``y``.\"\"\"\n",
    "        # TODO add support for sample weights\n",
    "        return loss(y_lower, y_higher, y_pred.ravel(),dist, sigma,metrics)\n",
    "\n",
    "    def negative_gradient(self, y_lower, y_higher, y_pred,dist,sigma,k=0,sample_weight=None, **kwargs):\n",
    "        \"\"\"Negative gradient of partial likelihood\n",
    "        Parameters\n",
    "        ---------\n",
    "        y : tuple, len = 2\n",
    "            First element is boolean event indicator and second element survival/censoring time.\n",
    "        y_pred : np.ndarray, shape=(n,):\n",
    "            The predictions.\n",
    "        \"\"\"\n",
    "        ret = negative_gradient(y_lower, y_higher, y_pred.ravel(), dist, sigma)\n",
    "        if sample_weight is not None:\n",
    "            ret *= sample_weight\n",
    "        return ret\n",
    "\n",
    "    def init_estimator(self):  # pragma: no cover\n",
    "        return ZeroEstimator()\n",
    "\n",
    "\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y_lower,y_higher,\n",
    "                                residual, pred, dist, sigma, sample_weight):\n",
    "        \n",
    "        \"\"\"Least squares does not need to update terminal regions\"\"\"\n",
    "        \n",
    "        \"\"\"Make a single Newton-Raphson step.\n",
    "        our node estimate is given by:\n",
    "            sum(w * gradient) / sum(w * hessian)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        hess            = np.array(hessian(y_lower, y_higher, pred, dist, sigma))\n",
    "        terminal_region = np.where(terminal_regions == leaf)[0]\n",
    "        residual        = residual.take(terminal_region, axis=0)\n",
    "        hess            = hess.take(terminal_region, axis=0)\n",
    "        sample_weight   = sample_weight.take(terminal_region, axis=0)\n",
    "        pred            = pred.take(terminal_region, axis=0)\n",
    "\n",
    "        numerator       = np.sum(sample_weight * residual)\n",
    "        denominator     = np.sum(sample_weight * hess)\n",
    "\n",
    "        # prevents overflow and division by zero\n",
    "        \n",
    "        if abs(denominator) < 1e-2:\n",
    "            tree.value[leaf, 0, 0] = 0.0\n",
    "        else:\n",
    "            tree.value[leaf, 0, 0] = numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_sample_mask(n_total_samples,n_total_in_bag, random_state):\n",
    "    \n",
    "    \"\"\"Create a random sample mask where ``n_total_in_bag`` elements are set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_total_samples : int\n",
    "        The length of the resulting mask.\n",
    "\n",
    "    n_total_in_bag : int\n",
    "        The number of elements in the sample mask which are set to 1.\n",
    "        \n",
    "    random_state : np.RandomState\n",
    "        A numpy ``RandomState`` object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_mask : np.ndarray, shape=[n_total_samples]\n",
    "         An ndarray where ``n_total_in_bag`` elements are set to ``True``\n",
    "         the others are ``False``.\n",
    "    \"\"\"\n",
    "    \n",
    "    #random_state = np.random.RandomState(random_state)\n",
    "    rand         = random_state.rand(n_total_samples)\n",
    "    sample_mask  = np.zeros((n_total_samples,), dtype=np.bool)\n",
    "    n_bagged     = 0\n",
    "    \n",
    "    for i in range(n_total_samples):\n",
    "        \n",
    "        if rand[i] * (n_total_samples - i) < (n_total_in_bag - n_bagged):\n",
    "            sample_mask[i] = 1\n",
    "            n_bagged += 1\n",
    "            \n",
    "    return sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference between Attributes and Parameters\n",
    "\n",
    "class BaseGradientBoosting():\n",
    "    \"\"\"Abstract base class for Gradient Boosting. \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, loss, learning_rate, n_estimators, criterion,\n",
    "                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n",
    "                 max_depth, min_impurity_decrease, min_impurity_split,\n",
    "                 init, subsample, max_features,\n",
    "                 random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,\n",
    "                 warm_start=False, presort='auto', validation_fraction=0.25,\n",
    "                 n_iter_no_change=None,metrics = 'logloss', Nestrov=False,dist='normal',sigma =1,\n",
    "                 tol=1e-4):\n",
    "        \n",
    "        #Initial = 1\n",
    "        self.n_estimators             = n_estimators + 1\n",
    "        self.learning_rate            = learning_rate\n",
    "        self.loss                     = loss\n",
    "        self.criterion                = criterion\n",
    "        self.min_samples_split        = min_samples_split\n",
    "        self.min_samples_leaf         = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.subsample                = subsample\n",
    "        self.max_features             = max_features\n",
    "        self.max_depth                = max_depth\n",
    "        self.min_impurity_decrease    = min_impurity_decrease\n",
    "        self.min_impurity_split       = min_impurity_split\n",
    "        self.init                     = init\n",
    "        self.random_state             = random_state\n",
    "        self.alpha                    = alpha\n",
    "        self.verbose                  = verbose\n",
    "        self.max_leaf_nodes           = max_leaf_nodes\n",
    "        self.warm_start               = warm_start\n",
    "        self.presort                  = presort\n",
    "        self.validation_fraction      = validation_fraction\n",
    "        self.n_iter_no_change         = n_iter_no_change\n",
    "        self.tol                      = tol\n",
    "        self.Nestrov                  = Nestrov\n",
    "        self.dist                     = dist\n",
    "        self.sigma                    = sigma\n",
    "        self.metrics                  = metrics\n",
    "\n",
    "    #Very Important loss class is defined here.\n",
    "    \n",
    "    def _init_state(self):\n",
    "        \n",
    "        self.estimators_    = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.fitted_        = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.prev_valid_    = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.train_score_   = np.zeros((self.n_estimators, ),dtype=np.float64)\n",
    "        self.valid_score_   = np.zeros((self.n_estimators, ),dtype=np.float64)\n",
    "        self.random_state   = check_random_state(self.random_state)\n",
    "        \n",
    "        if self.Nestrov == True:\n",
    "            \n",
    "            self.g_fitted_      = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "            self.g_prev_valid_  = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "            self.lamb           = np.zeros((self.n_estimators,),dtype=np.float64)\n",
    "            self.gamma          = np.zeros((self.n_estimators,),dtype=np.float64)\n",
    "            self.gamma[0]       = 1\n",
    "            \n",
    "            for i in range(1,self.n_estimators):\n",
    "                self.lamb[i] = 0.5*(1+math.sqrt(1+4*self.lamb[i-1]**2))\n",
    "                \n",
    "            for i in range(1,self.n_estimators-1):\n",
    "                self.gamma[i] = (1-self.lamb[i])/self.lamb[i+1]\n",
    "                \n",
    "        \n",
    "        #do oob?\n",
    "        if self.init is None:\n",
    "            self.init_ = self.loss_.init_estimator()\n",
    "        elif isinstance(self.init, str):\n",
    "            self.init_ = INIT_ESTIMATORS[self.init]()\n",
    "        else:\n",
    "            self.init_ = self.init\n",
    "\n",
    "        \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n",
    "\n",
    "        if self.subsample < 1.0:\n",
    "            self.oob_improvement_ = np.zeros((self.n_estimators),dtype=np.float64)\n",
    "    \n",
    "    def _check_params(self):\n",
    "        \n",
    "        \"\"\"Check validity of parameters and raise ValueError if not valid. \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.loss == 'aft':\n",
    "            self.loss_ =  AFT(1)\n",
    "            \n",
    "\n",
    "    def _fit_stage(self, i, X, y_lower, y_higher, sample_weight, sample_mask, random_state):\n",
    "        \n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
    "        \n",
    "        assert sample_mask.dtype == np.bool\n",
    "        loss       = self.loss_\n",
    "        #original_y = y_lower\n",
    "        pred       = np.zeros((X.shape[0],self.loss_.K),dtype=np.float64)\n",
    "        \n",
    "        for k in range(loss.K):\n",
    "            if self.Nestrov == True:\n",
    "                pred[:,k] = self.g_fitted_[i-1,k]    \n",
    "            else:\n",
    "                pred[:,k] = self.fitted_[i-1,k]\n",
    "        \n",
    "        for k in range(loss.K):\n",
    "   \n",
    "            residual = loss.negative_gradient(y_lower,y_higher,pred,self.dist,self.sigma,k=k,sample_weight=sample_weight)\n",
    "        \n",
    "            # induce regression tree on residuals\n",
    "            tree     = DecisionTreeRegressor(\n",
    "                                            criterion                 = self.criterion,\n",
    "                                            splitter                  = 'best',\n",
    "                                            max_depth                 = self.max_depth,\n",
    "                                            min_samples_split         = self.min_samples_split,\n",
    "                                            min_samples_leaf          = self.min_samples_leaf,\n",
    "                                            min_weight_fraction_leaf  = self.min_weight_fraction_leaf,\n",
    "                                            min_impurity_decrease     = self.min_impurity_decrease,\n",
    "                                            min_impurity_split        = self.min_impurity_split,\n",
    "                                            max_features              = self.max_features,\n",
    "                                            max_leaf_nodes            = self.max_leaf_nodes,\n",
    "                                            random_state              = random_state,\n",
    "                                            presort                   = self.presort\n",
    "                                            )\n",
    "\n",
    "            if self.subsample < 1.0:\n",
    "                # no inplace multiplication!\n",
    "                sample_weight = sample_weight * sample_mask.astype(np.float64)\n",
    "                \n",
    "\n",
    "            tree.fit(X, residual, sample_weight=sample_weight)\n",
    "            \n",
    "            #dot_data = StringIO()\n",
    "            #export_graphviz(tree, out_file=dot_data,  \n",
    "            #                filled=True, rounded=True,\n",
    "            #                special_characters=True)\n",
    "            #graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "            #Image(graph.create_png())\n",
    "            #filename = \"tree\"+str(i)+\".png\"\n",
    "            #graph.write_png(filename)\n",
    "\n",
    "            # update tree leaves    \n",
    "            if self.Nestrov == True:\n",
    "                \n",
    "                y_pred                  = self.g_fitted_[i-1,k]\n",
    "                self.fitted_[i,k]       = loss.update_terminal_regions(tree.tree_, X, y_lower, y_higher, residual, y_pred,self.dist,self.sigma,sample_weight, sample_mask,self.learning_rate)\n",
    "                self.g_fitted_[i,k]     = (1-self.gamma[i-1])*self.fitted_[i,k]+self.gamma[i-1]*self.fitted_[i-1,k]\n",
    "                \n",
    "            else:\n",
    "                y_pred            = self.fitted_[i-1,k]\n",
    "                self.fitted_[i,k] = loss.update_terminal_regions(tree.tree_, X, y_lower,y_higher, residual, y_pred,self.dist,self.sigma,sample_weight, sample_mask,self.learning_rate)\n",
    "\n",
    "            # add tree to ensemble\n",
    "            self.estimators_[i, k] = tree\n",
    "    \n",
    "    def n_features(self):\n",
    "        return self.n_features_\n",
    "    \n",
    "    def _validate_y(self, y, sample_weight):\n",
    "        self.classes_    = np.unique(y)\n",
    "        self.n_classes_  = len(self.classes_)\n",
    "        return y\n",
    "    \n",
    "    def _fit_stages(self, X, y_lower, y_higher,sample_weight, random_state,\n",
    "                    X_val, y_lower_val, y_higher_val,sample_weight_val,begin_at_stage=0):\n",
    "        \n",
    "        \n",
    "        n_samples    = X.shape[0]\n",
    "        do_oob       = self.subsample < 1.0\n",
    "        sample_mask  = np.ones((n_samples, ), dtype=np.bool)\n",
    "        n_inbag      = max(1, int(self.subsample * n_samples))\n",
    "        loss_        = self.loss_\n",
    "        \n",
    "        # create one-hot label encoding\n",
    "        pred         = np.zeros((n_samples, self.loss_.K), dtype=np.float64)\n",
    "        pred_val     = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "        \n",
    "        for k in range(self.loss_.K):\n",
    "            pred[:,k] = self.fitted_[0,k]\n",
    "            pred_val[:,k] = self.prev_valid_[0,k]\n",
    "            \n",
    "        if do_oob:\n",
    "            \n",
    "            sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n",
    "            self.train_score_[0] = loss_(y_lower[sample_mask],y_higher[sample_mask],pred[sample_mask],self.dist,self.sigma,self.metrics,sample_weight[sample_mask])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.train_score_[0] = loss_(y_lower,y_higher,pred,self.dist,self.sigma,self.metrics,sample_weight)\n",
    "        self.valid_score_[0] = loss_(y_lower_val,y_higher_val,pred_val,self.dist,self.sigma,self.metrics,sample_weight_val)\n",
    "\n",
    "        # perform boosting iterations\n",
    "        # validation loss performance\n",
    "        \n",
    "        for i in range(begin_at_stage, self.n_estimators):\n",
    "\n",
    "            # subsampling\n",
    "            if do_oob:\n",
    "                sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n",
    "                \n",
    "            # fit next stage of trees\n",
    "            self._fit_stage(i, X, y_lower,y_higher, sample_weight,sample_mask, random_state)\n",
    "\n",
    "            if self.Nestrov == True:\n",
    "                \n",
    "                score = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "                \n",
    "                for k in range(self.loss_.K):\n",
    "                    score[:,k] = self.g_prev_valid_[i-1,k].copy()\n",
    "                    \n",
    "                predict_stage(self.estimators_, i, X_val, self.learning_rate, score)\n",
    "\n",
    "                for k in range(self.loss_.K):\n",
    "                    self.prev_valid_[i,k] = score[:,k].copy()\n",
    "                    \n",
    "                for k in range(self.loss_.K):\n",
    "                    self.g_prev_valid_[i,k] = (1-self.gamma[i-1])*self.prev_valid_[i,k]+self.gamma[i-1]*self.prev_valid_[i-1,k]\n",
    "            else:\n",
    "                \n",
    "                score = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "                for k in range(self.loss_.K):\n",
    "                    score[:,k] = self.prev_valid_[i-1,k].copy()\n",
    "\n",
    "                predict_stage(self.estimators_, i, X_val, self.learning_rate, score)\n",
    "                \n",
    "                for k in range(self.loss_.K):\n",
    "                    self.prev_valid_[i,k] = score[:,k].copy()\n",
    "\n",
    "            for k in range(self.loss_.K):\n",
    "                \n",
    "                pred[:,k] = self.fitted_[i,k]\n",
    "                pred_val[:,k] = self.prev_valid_[i,k]\n",
    "            \n",
    "            if do_oob:\n",
    "                self.train_score_[i] = loss_(y_lower[sample_mask],y_higher[sample_mask],pred[sample_mask],self.dist,self.sigma,self.metrics,sample_weight[sample_mask])\n",
    "            else:\n",
    "                self.train_score_[i] = loss_(y_lower,y_higher,pred,self.dist,self.sigma,self.metrics,sample_weight)\n",
    "   \n",
    "            self.valid_score_[i] = loss_(y_lower_val,y_higher_val,pred_val,self.dist,self.sigma,self.metrics,sample_weight_val)\n",
    "    \n",
    "        return i + 1\n",
    "\n",
    "    \n",
    "    def fit(self, X, y_lower,y_higher, X_val = None,y_lower_val=None, y_higher_val=None,sample_weight=None):\n",
    "        \n",
    "        # Check input\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        #y_lower                     = self._validate_y(y_lower, sample_weight)\n",
    "        #y_higher                    = self._validate_y(y_higher, sample_weight)\n",
    "        X                           = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        sample_weight               = np.ones(n_samples, dtype=np.float32)\n",
    "        \n",
    "        if X_val is None:\n",
    "            X, X_val, y_lower, y_lower_val,y_higher,y_higher_val,sample_weight, sample_weight_val \\\n",
    "            = train_test_split(X, y_lower,y_higher,sample_weight,random_state=self.random_state,test_size=self.validation_fraction)\n",
    "            self._check_params()\n",
    "            self._init_state()\n",
    "        else:\n",
    "            sample_weight_val = np.ones(X_val.shape[0], dtype=np.float32)\n",
    "\n",
    "        # fit initial model - FIXME make sample_weight optional\n",
    "        #For Binomial       - init_ = LogOddsEstimator\n",
    "        #For Multinomial    - init_ = PriorProbabilityEstimator\n",
    "\n",
    "        self.init_.fit(X, y_lower,y_higher, X_val, y_lower_val,y_higher_val, sample_weight)\n",
    "        # init predictions and update in the inplace self\n",
    "        initial_pred,initial_val_pred  = self.init_.predict(X,X_val)\n",
    "        \n",
    "        for k in range(self.loss_.K):\n",
    "            self.fitted_[0,k], self.prev_valid_[0,k]          = initial_pred[:,k],initial_val_pred[:,k]\n",
    "            if self.Nestrov == True:\n",
    "                self.g_fitted_[0,k], self.g_prev_valid_[0,k]  = initial_pred[:,k],initial_val_pred[:,k]\n",
    "\n",
    "        begin_at_stage = 1\n",
    "        # fit the boosting stages\n",
    "        \n",
    "        n_stages = self._fit_stages(X, y_lower,y_higher, sample_weight, self.random_state,\n",
    "                                    X_val, y_lower_val,y_higher_val, sample_weight_val,begin_at_stage)\n",
    "        # change shape of arrays after fit (early-stopping or additional ests)\n",
    "        self.n_estimators_ = n_stages\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _make_estimator(self, append=True):\n",
    "        # we don't need _make_estimator\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def _init_decision_function(self, X):\n",
    "        \n",
    "        \"\"\"Check input and compute prediction of ``init``. \"\"\"\n",
    "        #self._check_initialized()\n",
    "        #X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
    "                self.n_features_, X.shape[1]))\n",
    "        score = self.init_.predict(X).astype(np.float64)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _decision_function(self, X):\n",
    "        \n",
    "        # for use in inner loop, not raveling the output in single-class case,\n",
    "        # not doing input validation.\n",
    "        \n",
    "        score = self._init_decision_function(X)\n",
    "        predict_stages(self.estimators_, X, self.learning_rate, score)\n",
    "        return score\n",
    "\n",
    "    def _staged_decision_function(self, X):\n",
    "        \n",
    "        #X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        \n",
    "        score = self._init_decision_function(X)\n",
    "        for i in range(self.estimators_.shape[0]):\n",
    "            predict_stage(self.estimators_, i, X, self.learning_rate, score)\n",
    "            yield score.copy()\n",
    "    \n",
    "\n",
    "\n",
    "class GradientBoostingClassifier(BaseGradientBoosting):\n",
    "\n",
    "    _SUPPORTED_LOSS = ('survival')\n",
    "\n",
    "    def __init__(self, loss='aft', learning_rate=0.1, n_estimators=100,\n",
    "                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n",
    "                 max_depth=3, min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None, init=None,\n",
    "                 random_state=None, max_features=None, verbose=0,\n",
    "                 max_leaf_nodes=None, warm_start=False,\n",
    "                 presort='auto', validation_fraction=0.25,\n",
    "                 n_iter_no_change=None,Nestrov=False,metrics = 'logloss',dist='normal',sigma=1,tol=1e-4):\n",
    "\n",
    "        super().__init__(\n",
    "            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n",
    "            criterion=criterion, min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_depth=max_depth, init=init, subsample=subsample,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state, verbose=verbose,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            warm_start=warm_start, presort=presort,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change,Nestrov=Nestrov,metrics=metrics,\n",
    "            dist=dist,sigma=sigma,tol=tol)\n",
    "\n",
    "    def _validate_y(self, y, sample_weight):\n",
    "        #check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n",
    "        if n_trim_classes < 2:\n",
    "            raise ValueError(\"y contains %d class after sample_weight \"\n",
    "                             \"trimmed classes with zero weights, while a \"\n",
    "                             \"minimum of 2 classes are required.\"\n",
    "                             % n_trim_classes)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        return y\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        score = self._decision_function(X)\n",
    "        if score.shape[1] == 1:\n",
    "            return score.ravel()\n",
    "        return score\n",
    "\n",
    "    #def staged_decision_function(self, X):\n",
    "    #    \n",
    "    #    yield from self._staged_decision_function(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "       \n",
    "        score     = self.decision_function(X)\n",
    "        decisions = self.loss_._score_to_decision(score)\n",
    "        return self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def staged_predict(self, X):\n",
    "       \n",
    "        for score in self._staged_decision_function(X):\n",
    "            decisions = self.loss_._score_to_decision(score)\n",
    "            yield self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        score = self.decision_function(X)\n",
    "        try:\n",
    "            return self.loss_._score_to_proba(score)\n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \n",
    "        proba = self.predict_proba(X)\n",
    "        return np.log(proba)\n",
    "\n",
    "    def staged_predict_proba(self, X):\n",
    "       \n",
    "        try:\n",
    "            for score in self._staged_decision_function(X):\n",
    "                yield self.loss_._score_to_proba(score)\n",
    "                \n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "def data_creation(file_name,K=5):\n",
    "    \n",
    "    df_train   = pd.read_csv(file_name)\n",
    "    y          = list(df_train['Y'])\n",
    "    req_cols   = [i for i in df_train.columns if i != 'Y']\n",
    "    X          = np.array(df_train[req_cols])\n",
    "    y_median   = np.percentile(y, 50) # return 50th percentile, e.g median.\n",
    "    bin_y      = list(map(lambda x : 0 if x < y_median else 1,y))\n",
    "\n",
    "    percentile = np.percentile(y, np.arange(0, 100, 100/K)) # deciles\n",
    "    multi_y    = list(map(lambda x : 0 if x >= percentile[0] and x< percentile[1] else 1 if x >= percentile[1] and x< percentile[2] else 2 if x >= percentile[2] and x< percentile[3] else 3,y))\n",
    "    \n",
    "    return X,y,bin_y,multi_y\n",
    "\n",
    "def chart_creation(gb,chart_title,chart_name):\n",
    "    \n",
    "    min_valid = round(np.min(gb.valid_score_),4)\n",
    "    min_train = round(np.min(gb.train_score_),4)\n",
    "    min_iter  = round(np.nanargmin(gb.valid_score_),0)\n",
    "\n",
    "    textstr = '\\n'.join((\n",
    "                    'Min Train = %.2f' % (min_train, ),\n",
    "                    'Min Valid = %.2f' % (min_valid, ),\n",
    "                    'Min Iter  = %.2f' % (min_iter, )))\n",
    "\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5,edgecolor=\"black\")\n",
    "    \n",
    "    fig,ax1       = plt.subplots()\n",
    "    ax2           = ax1.twinx()\n",
    "\n",
    "    ln1 = ax1.plot(gb.train_score_,color='blue',label='Training')\n",
    "    ln2 = ax2.plot(gb.valid_score_,color='orange',label='Validation')\n",
    "    \n",
    "    #ax1.axvline(x=np.nanargmin(gb.valid_score_),color='r')\n",
    "    #ax2.axhline(y=np.min(gb.valid_score_),color='b')\n",
    "    lns = ln1 + ln2\n",
    "    \n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='lower left',fancybox='round', facecolor='wheat',fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel(\"Number of Iterations(Trees)\")\n",
    "    ax1.set_ylabel(\"Training Negative Likelihood(Loss)\")\n",
    "    ax2.set_ylabel(\"Validation Negative Likelihood(Loss)\")\n",
    "    #ax1.legend([\"Training\",\"Validation\"],loc='lower left',fancybox='round', facecolor='wheat',fontsize=8)\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax2.text(0.7, 0.90, textstr, transform=ax1.transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "    plt.title(chart_title)\n",
    "    plt.show()\n",
    "    fig.savefig(chart_name)\n",
    "    \n",
    "def generate_result(X,y_lower,y_higher,param,X_val=None,y_lower_val=None,y_higher_val=None):\n",
    "    \n",
    "    gb_manual = GradientBoostingClassifier(**param)\n",
    "    gb_manual.fit(X,y_lower,y_higher,X_val,y_lower_val,y_higher_val)\n",
    "    \n",
    "    return gb_manual    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbkAAAEWCAYAAAD7HukTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzsnXmcTeX/wN8fBjOMse/7LhJZspR9F5EiJAqlUlT6Viq/Uvm2UIoWKZL62pfs2cmSnazJkGXs6xjbmBmf3x/PuVwzd+Zes5jF8369zmvuebbzOcd1Pvd5ns8iqorFYrFYLGmRdMktgMVisVgsSYVVchaLxWJJs1glZ7FYLJY0i1VyFovFYkmzWCVnsVgsljSLVXIWi8ViSbNYJWexWNI0IrJHROomwbirROTpxB7XkrhYJZfCEZEDInJSRLK4lfUSkeUJGLOBiIQkioBJhIjsFJGLzhElIlfdzt++A9f/VUTeT+rr+IqI+ImIisgxEUnvVp5RRM6ISGQSXLO0c03Xcz8uIrNFpPFtjJGg76oPsm2IVp5PRCJEJNhVpqrlVHVlYstgSR1YJZc6SA/0S24hXIiIX1JfQ1UrqmqgqgYCK4GXXOeq+t/kkCmFcAFo5nbeGjidlBd0+3e4H1gKzBKRrkl5zdsgSETucTt/EtifXMJYUh5WyaUOhgCvi0j26BUiUl5EFonIWWdZpqNbXSsR2SUiYSJyRERed2aE84GCbr/QC4pIOhF5S0T2OTODySKS0xmnuPOruaeIHMK86BCRR5wZ13kRWe562YjImyIyNZqcX4nI8MR6IM4M4Q8RGS4iZ4F3ReQjERnr1qa0iKjbeXYR+cmZDYWIyAcictv/B0TkIRHZKCKhIrJeRGq61fV0Zt9hIrJfRDo55WUdeUNF5LSIjI/nrf8CdHM77waMiyZfLxHZ7ciwT0R6Rat/RET+cv7dVonIvb5cWFWPqeow4EPgMxERZ7x3nXsNc74PjzjllYCvgbrO9+y02/W3isgFETkkIgPj+SzAt+cR4qxeiIgsEJFP3eqmisgot/NeIvK3iJwTkfkiUsStroXzfyxURL4CJAFyW+4UqmqPFHwAB4AmwHTgI6esF7AcyAIcBp4B/DC/tE8DFZx2x4C6zuccQFXncwMgJNp1+gFrgcJAJuB7YIJTVxxQzMsjCxAAlAUuAU2BDMAbQDCQESgGXAayOv3TO7LUcs6/Bc7Hcmzz8AyWA72ilfUCIoEXnPEDgI+AsW5tSpuv+I3z2c61MwP5gE1Az1ie+6/A+x7KcwOhQGfnmT8FnHGeb5BTV8ZpW8Dt32IK8Cbmh6U/8KDbmDvjeB7DnTZ+zr9BBeCEc61cwHHgPiDSbbw2QEnMS7gRcAW4z6mr4fSv4Ty3HsA+IKOHe73l+bmVl3Vkcd1nR+de0wFdgItAPvfvarT+jYCKTvvKmO9sa7fvSmzP4jzwurtsmO/aQWesSsAOoAUQ7Ha9EKCB87kgcAqoB3THfGezOHWPAXuAcs7zfh9Y6dTlde7rUcz3/T+Y79/Tyf2OsEfcR7ILYA8v/0A3ldy9zgs0DzeV3BOu/4Ru7b8H3nM+HwJ6A0HR2jQgppLbDTR2Oy8ARDj/2Ys7L5SSbvUDgclu5+mAI24vk1VAN+dzU2BfAp7Bcjwruf3RymJVckAhzMs+k1v9U8CiWK4Zm5J7BlgTrWwD0BWjeM47L0L/aG3GA98BheL5DFxKrjgwFugJvOSMWR43Jeeh7xygj/P5B9f3w61+H25K19Pzi1Ye6MhSM5br7QAedvt3Wu7l3r4Ghtzm83D/t10ONAaGYn5IxKrknPMnnP8bZ4DabuWLgO7Rnnm4893pAayK9n0/hlVyKf6wy5WpBFXdgXlZveVWXAyo6Sw7nReR85g9ifxO/WNAK+CgiKwQkdpxXKIYMMNtnN1AFGbG4+Kw2+eCmF/QLvmuO/WFnKLxmNkOmF/38V2ei4vD3pvcoBhmhnrC7R6/4db784Vb7tvhIEZ5XcDccx/guIjMEZGyTpv+mBnARhHZLiLdb/O67ozDLMvFWJoDEJHWIrLOWcI+j9nDy+1UFwPejPadKcDNfzdfcLU961zvabflz/MYpZs7ts4iUttZ3j4lIqEYRRhrex8Yh/nx0Qnz48QbMzHfhR2q+qdbeTHgG7f7OA1cx6xuFMTt++Z831O08ZbFYJVc6uI94FluvmQOAytUNbvbEaiqLwCo6gZVbYtZavkNmOz085R64jDQMtpY/qp6xK2Ne7+jmJcCAM7+TBHMbA7M8lwDESmMmdmMd2s7Um7uB0Y/dt7G84h+H5cwS5Eu8rt9PoxZQs3pdn9BqnrfbVwPot23Q1Gc+1bV+araBKM4gjEza9TsZ/VS1QIYJThKRErADRP32J7H1x5kWObIkD3aSxoRCQCmAh9jlgyzAwu5uX90GBgU7d85s6pOxncexSyTBotIScxs8gUgl3O9v92u5+m7NhGYBhRR1WzAj672IpI+jmdxUUTe8DDeFKAdsDva9zU2Pgb+AoqLSAe38sOY5Wv3ZxOgquswszb3/bl0GOVnSeFYJZeKUNVgYBLQ1ymaA5QVkadEJINz1BCRe8SYlj8pItlUNQJjlXfd6XcCyCUi2dyGHwkMFpFiACKSR0TaxiHOZOBhEWksIhkwM5VwYI0j6ynMMtJPwL+qutvtPp7Xm5aS0Y+KCXhEW4H6IlJEjJHOjVmvqh4GVgBDRSRIjKFNaRGp59yvyyTd/cXlJyL+bkdGzDOvKCJPiDHr74JZOpsrIgVEpI2IZAauYZTudWf8jiLi+nFyHvPyj3JkKxfH83gp+k2qqmKsKtt5eAaZMPuip4AoEWmNWcpz8QPQx/meiIgEOjJnceT8VUR+9PRwxZjn9wXeBd505HAtXZ4yTeRZzEzOxQmgsPMdcZEVOKuqV0WkFmYG5rq3qDieRaCqfubheYQBDTFL83EiIo0wqx3dMHty34pIAad6JPCO3DSgyi4ijzt1c4AqItLWuZdXMVsHlhSOVXKpjw8wxh+u/9zNMC+Jo5hf159iXnRg9pwOiMgF4HnMf25U9W9gArDfWZopCHwFzAIWikgYxgjlhtVgdFR1D2YfagRmWacN0EZVr7k1G4/ZT0yKpUpP/A7MALYD6zH3405XzLPbBZzDzABcs70iGNPz427t38Hs47mOhY7yfgSz93MG87JrrarnMEYT/8H86j8D1MHM2sA8yw0icgljRNRHVQ/F90ZVdYeq7vJQft6RaQZmOfFxzAvaVb8WM+v6znkG/zjPxUURYLX7mK5ZFLANaA60V9VxznjbMN+B9c59lwPWuXVfBOzFLBO7nu0LwMfO9+xtbq4wxBtn1SJO1wHnh89Y4EVVPa6qyzFLnaOdMaYAXwBTnP8zrvtFVU9g9vKGYL7vRaPdpyWFIubHmMVydyPG8fuwqo5OblmSCxHxBzZjLDET3bncYkkOrJKzWCwWS5rFLldaLBaLJc1ilZzFYrFY0ixWyVksFoslzZJmg9qmS5dOAwICklsMi8ViSTVcvnxZVTVNTX7SrJILCAjg0qVLyS2GxWKxpBpE5Epyy5DYpCmNbbFYLJakQ0TGiMlvuSNa+ctisjfsFJHP3MoHiEiwE9WnuVu5K6NDsIi4hypMdNLsTM5isVgsic5YTEDtGzFTRaQh0BaorKrhIpLXKa+ACVRRERP7c7HcjOX6DSZwewgmSMIsT8ENEgOr5CwWi8XiE6r6h4gUj1b8AvCJqoY7bU465W2BiU75v2KytT/g1AW7ItSIyESnbZIoObtcabFYLBYXfmISAruO53zoUxaTGHedmGwnNZzyQtyaKSTEKYutPEmwMzmLxWKxuIhU1eq32ccPyAnUwiTjnexkp0gRWCVnsVgsloQQAkx3slKsF5HrmPyAR3BLT4RJTeRKhRRbeaJjlystFovFkhB+w6Q6wjEsyYjJ1DAL6CQimcTkTiyDyVaxASgjIiWc9FWdiJkxJNGwSs5isVhSGVu2wKJFMcv15Cp0Z4yUe4mGiEwA/gTKiUiIiPQExgAlHbeCiUB3NezEpFHahUmD1cfJFxgJvAQsAHYDk522SSNzWs1CkCVLFrXO4BaLJS3SqBGsWQO7dkHJkkBUOGz7P3TXEI5fKknQE3+RJVuW2x5XRC6r6u13TMHYmZzFYrGkIlTNTC48HF55BTi3FX6vDrs/Y/SKZ+kzewuZg9KUnkoQ1vDEYrFYUhEHDsD581ClchTlrn/B9fnvIP656Td9LhNWtGL7dhBJbilTDlbJWSwWSypiyxYokusQywd2J1v4cubvfIzV175nxLRczJgB+fMnt4QpC6vkLBaLJRURuW8C2z5+gaCoKHYF/USr/3YHhF69oF275JYu5ZFke3IiUkRElonILidoZz+nfIgTyHObiMwQkexOeXERuSIiW51jpNtY1URkuxPMc7iInYxbLJa7jGvnYU1XOhbqwoFzFZCWW6nQ+ml69BAqVYJhw5JbwJRJkllXikgBoICqbhaRrMAmoB3G8W+pqkaKyKcAqvqmEw9tjqre62Gs9UBfYB0wDxiuqvPjur61rrRYLKmRkBDw94fcud0Kjy+Btc/AlaMM+f09dugAfh5nFuJU4fp1SJ8+4de21pW3gaoeU9XNzucwjD9EIVVd6PhJAKzFKL1YcZRlkKqudTzqx2GUpcVisaQpVKFhQ6haFU6eBCIvw8Z+sLQJ+GXmbPU1vDFuIJWr3NxpEkkcBZdWuSMuBM4s7X7MTMydHoD7jKyEiGxxgnzWdcoKYcLGuIg1mKeIPOcKLBoZGempicVisaRYNm+G4GA4fBgG9N6Ezq8G/wyHcv2gxWY2/muC+N9/fzILmopIcsMTEQkEpgGvqOoFt/J3gEjgf07RMaCoqp4RkWrAbyJS8XaupaqjgFFglisTQ36LxZL4rF+/jp07thEenuYSUcfJpo1QqBDkL+C5fvEiyJ9H6fnISgqmX8ZXs7LgX7ofhJaE9T+yelUGMmQoxr33Ngfs9M0XklTJiUgGjIL7n6pOdyt/GmgNNHaWIHFyDrnyEW0SkX2YFA5HuHVJM0mDeVoslqRl9epVbFizjGaNapI5wD+5xbljXLwIuzZB7izQor6HBgoXj4QyoP3v5MocyenwXkz6owl1i2XivvtMk2uhkVw6v51FC8/QuUtX7iYbPBHJgUm+egU4oKrXfemXZErOsYAcDexW1S/cylsAbwD1VfWyW3ke4KyqRjlpGsoA+1X1rIhcEJFamOXObsCIpJLbYrEkLWtXr6DTo03IkztncotyR9mwHrIGGkfuAvmjOWwrhB3awguNfiedXzrSF+lKgaBKbD8NmzdBnVoQlA0iI6Dtw/k5cmgyoaGhZM+ePdnu504gItmAPkBnTODnU4A/kE9E1gLfquqyuMZIyj25B4GngEZubgGtMKnTswKLorkK1AO2ichWYCrwvKqedepeBH4EgoF93LqPZ7FYUhGXLl8ie7asyS3GHWfXbvP3yhU4ecKtIuIiHJpI1guzCDlXiPAiL0D2Skg6aN0aUFiwACKuwdkzUKhgegKzBHCXWI9PxSRYrauq5VT1IVWtrqpFgE+Atk6Q6FhJspmcqq4CPM2l58XSfhpmadNT3UYghmuBxWJJ3axYtY6Wj/bgwK4/yJsnFxs3b+fBJh3Ys3Uxe/75l6jrUbRq1iDW/lOmz2PUTxM4fvI0qkqBfHl47pnOdGjfKs7r/vTLVKpWqUjlSvck+B7GjJvCsG/GkD9vbmrWqMJH/9cfgE+/GMmyleuIioxk5sRf2L8/glUbBnDg4FHOhjXmq6E9IXQ3HJ0DUdeYuq4aH/80i1y5FtO1Uzu6dWnP2dAQ5i//kJ8mXWHz9nb4+7UnfwEIPnx3LFOqatM46jZhXNPixEY8sVgsyUrlSuWZPW8JPbt3ZObcRVS73/yebd6krpee0KF9Kzq0b8W48dOJjIyiR7cOt9Rfv36ddOliLlg989TjiSO8w3/6PUu3Lu1vnP+5bjMREZH8PuMnALZshr+DF/NQnXI82moo4yb24NRfGciT7l8IKEhozkcZ/OPHfPrhpzRrmuPGOO99NIxp//uEyZNzcOaMKStwF4btEpEHga2qeklEugJVga9U9aC3vnEuV4pIDRH5SkQ2i8gxEdkvIrNEpLfj4G2xWCwJokHdWiz7Yy0Au/4OpkL50gCMGz+dMeOmcOBQCI1aPckT3V6mVsP2hBw57nXMh5p05MVX/48B7w1h3sLlNG3zFLUbPcaEKbMBeG/wl6xYtY4ly9fwSMfnaNepN41aPcnly/Gz9vzym59o0rorK1YZL6l5C5dz4tQZmj3SjU+/GMmu3XDm3Fbata5DzXuCaVsrko3r10C+hlCqJ5t2ZiXs4gm+Gz2Q1o/3Inj/Qa5eDefI0RP06T+QGfN7cfb8QTJnhqx355v3O+CyiFQG+mO2rcb50jHWmZyIzMVkd50JfA6cxGz4lcVkgZ0rIp+p6pyEyW6xWO5mMmbMgL9/RtZt2Er5sqU4cfJUjDYXL11m8ZxfmDRtLjNmL+Tl57vFOebJ02d4940+FCyQj8uXr9CqWQMiIiJo9kh3Ondoc0vbAP9MTBo3go8+/Zrlq9bdsjz6wcfDWblmwy3t3/5PHxrWq3XjvH3b5jzz1OOcPHWGNh2eZe2yaZw8eYaCBfIxYtY4Oj7Vl5AcewjwDyXrlQ2UznuQlf5ZOBRVB/LWQxXWrjvLmXN7+eGb3zl2/CTvvD+Uof99m52797Jjgyl76bWhtGw+wvMmUNonUlVVRNoCX6vqaG97cS7iWq7soaonopVdxaQvXw98KiJ54yevxWKx3KRFk/q81P99vh32Ad+PGR+j/p5ypUiXLh0FC+Rl336vK1QUyJeHggXyAbBxy3YGD/mWqMhI9uzdH6NtxXvKAFCwQD5CQ8Nuqfu/AX29Xit7tiAAMvvnpkSxwpw+c46goEDqPlgDgLKla3L+2FrK5NlP2NGrRD7QmsXb9tM4d1EAtm6Fi2FZKVu6NLlz5SB3rhycOn2WbEFZqVD+Zln69Gdp0cKrOGmVMBEZAHQF6olIOiCDLx1jXa50KTgRCXAFRBaRUiLSSkT8nDYnEyy6xWK562nRtB5VK1eketVKHuvd/cF8ibfrvg835MsfGP3NJ8ybPoasWQNva+wPPh5O0zZP3XK4llZdXLhwkUsXYejnV9i2/Qi5cman9gP3s33nHrgeyf6di+neaBN17svP0gNF8CvShGMnN+Gf8V7Cr8KSxVCmTCA5cmTk6tVwDoUcJVu2rAQFBZIx061ldzFPYPyoe6rqcYy/9BBfOvpieLISozmzAUuBzUAnjL+axWKxJJjAwCx8P2JwkozdrnVTHu38PJUrlb8x6/KV/xvQl6tXYOFCqFMHcueJ2WbY12OYNW81Z88otar1Zteu9LRp1Zjn+7xOo8bjqFg8M/mKdqTyI/Xp+dK7NGzZhRpVG3L+XC6+/2EHf278h2+/ak/xUr1p1b4HUVFRfPnZQADefDVm2V1KGMbQJEpEygLlgQm+dPSahUBENqtqVRF5CQhU1U9EZKuqVkmw2EmIzUJgsaRMPnj/bfq/2IUMGXxabUp21v4JCxZCjuzw7LMQkDlmm6lT4NAhyJkLjh1V+nZeTeaLy7l4NTMr97ej0SMl8Q+42f6fPTBhovlcvTo8/PDtyTTm15m0af8khQp5DOMbb1JqFgIR2QTUBXIAq4ENwDVVfdJbX1+cwdOJSA3gScBlZGKDplkslrSPmj2zHDkg7CJMmgxRUbc2uX4d9u2D0mWgY9tzdHtoLFkuLmH3kfLM3PkCjdvequAAihUzEU8CAqBRwzt3O6kYcSJktcdEOemAj77TvixXvgYMwuR62+GE3FoZb1EtFosllXD8BJw4aWZa/v4wbRrMmwtt2nDDyjEkBK6GQ41SW8l8dD7+OYXfNrTnPJXo0gUyZoo5biZ/aNgA8uXzPDO0xEBEpDZmsuWyqvQpYpfXRqq6VFVbqepgxwDlhKq+GH9ZLRaLxbBi1Toy56nIyVPG03nj5u1kylmeA4dCWLB4JfMWLo+z/569++n8dL8b51FRUdRp7NnR+8ChEJ7u/R8AXnnzwxj1Tds8FaPsr60mV9u9FeHee6FePdi8BbbvcBt33xXaVv0fNds+zZJt4aQr+wJzNk7hp4mdad6uizFAAcb+Oo2yVRrfkKFuPShbzozx70HjC9j44a50e7Y/UVFRhIVdpHm7p2n8cFfadepNWNjFOJ9FGucVYAAwQ1V3OpOtOGNWuvCq5ERknIgEiUhmYDsQLCKvJUhci8VicXBFPAFiRDyJK6QXQLkyJTkccoyrV8MBWLlmAw/Vru71ml9+6t2IIyoKtm2HcuW4sdzYoAEULGgsIiMjgLB9VA/6jlWb5lOxfEnI3xQyZuOdN55jxYIJjPr6v3z06dcAtG7ZiHnTx3i8VvZsWZkxcSRL5v5K8WKFmb9oBRkyZGDsyM9YMvdX2rRszLgJM7zKnFZR1RWq+gjwjYgEqup+VfXu34Fv0737nDxw7YBFQDHg6XhLa7FYLG4kNOJJ44Z1WLJiDQAz5y6mbeumRERE3JgFPdHtZaKibaQ1bNkFMDPHmg3a0+WZVzh3/sItbYKD4fJlqFL5ZpkINGsKF8OiOLltERz4lXNhGVi4LTt1Hmp4I7VAiWImO1iGDH6kd9J2586VAz8/z+YMObJnI1uQcRHI4Gf6+PtnokD+vDHGuRsRkUoisgXYCewSkU2+5hv1RcllcPzi2gIzVfUa4FMeH4vFYvFG9Ignnrh46TITxn5FvxefYcbshbfUtWvdlNlzzUxw3Yat1H7gfvz8/PhtgpkZlS9XKoZvm4vBQ75hyq9fM2rEYI4cvVV5bt0Kv81/mRdeu9VPLnPG/bzY4icKZlzDiYgavPxNbrp07uhx/IEffEGf3jGXQWPj6LETLFm+hqYNH7x57xcv8ePYSXR6vLXP46RBvgdeU9ViqloUE9rrB186+qLkfgQOYUw3V4hIUeCuXhy2WCyJiyviSbvWnoPOu0c8CQ29dcZ1f+WKbN+1h/Ub/6JypXtIly4dly5dpnffd2jSuivTZy3g2HHPcStCQ8MoWrgggYFZKFOq+I3yy5dg7z/w6QcjWDznFxbNdo5f3ybP+cnkyHKGKeueYOTsZhw4soZOj9eLMfbw737mnnKlebBWNZ+eQXj4NXq+OIDvvvwQPz9jE6iqPPfyOwx699Xb9vFLCkRkjIicFJEdHur6i4iKSG7nXERkuIgEi8g2Eanq1ra7iOx1ju4+XDqLe944VV0O+OTq4NW6UlWHAcPchDsMNPJlcIvFYvGFFk3rsWjpKqpXrcT3HratvEU8qf1AVd4ZNJT+fZ8FYNHSVZQpVZxxP3zO/300LNYoKUFBgYQcOU6O7EEEu8KFKcyfDwp8O/plhow4b04izkHEBcZ/1oE893UjYH82wvac4Gr4MVp36MW+fw8xf9EKqlapyMbN21m7fgv/GzPM43U98eKrA3m+VxfucZZrAQb9dzi1a1a9JVZmMjMWkxP0luDIIlIEaIaZELloiUl+XQaoiQmyXFNEcgLvAdUxT3aTiMxS1XNxXHe/iAwEfnHOuwIxY7R5wKuSc7INDMQkNQVYAXwEXPPlAhaLxeKNhEY8ademKT//bxqN6htlUKNaZT754ns2bd1BtqCslC5ZzGO/t19/kceefJEypYpTpHBBwBib7NgJjRvBwIEjIOICHJoKlw9DrprGuCRdeho0gJAj+fj9pakULQYffjKCOrWqkSN7Nl596yOCsgbS7JFulCldgm+HfcDcBcsY+uUP7D9wmCe6vcykcSMYN34691Yoy7VrEfw2ZxGHDh/l65E/81LvbtSodh9Dh/9I7QeqMGvuIh5/tBW9e3SO9zNKDFT1DxEp7qFqGPAGJqC/i7bAODW/MNaKSHYRKQA0ABa5kmKLyCKgBXFHMOmBcWWbjlGMK4FnfJHZl4gnU4B/gJ+doqeAe1Q1cRMyJTI24onFkjJJyRFPzp+Hkd9B/vzQvTvI5YNweApcj4BCbSF7heQWEUjSiCfXMFb0Lkap6qhobYpj/Kbvdc7bAo1UtZ+IHACqq+ppEZkDfOIk0EZElgBvYpScv6p+5JQPBK6o6tDblHWoqr7urZ0vzuBlHO9yFwNFZOvtCGOxWCwpnevXYcZ0QODRR0HOroPjCyFjDijRHfw9BK5Me0SqqncfDAfHtextzFLlnaYj4FXJ+WJ4clVEbiwIO5+vJkAwi8ViSVGEX4WJE+DQYWjzcATZwmbAsd8ha1ko9ezdouDiQymgBPCXM4srDGwWkfzAEaCIW9vCTlls5beLT5n1fFFyLwKjHQuZfRizzee9Xl2kiIgsE5FdIrJTRPo55TlFZJFjVbNIRHI45YlpiWOxWCw+cfo0/PAD7N8Pj7U5T8WAMXB+O+RrBEU7QnoPcbksAKjqdlXNq6rFVbU4EAJUddLhzAK6Oe/2WkCoqh4DFgDNRCSH8/5v5pTFwNEXno5cJJaSU9XNqloReACooaqVMA7h3ogE+qtqBaAW0EdEKgBvAUtUtQywxDmHWy1xnsNY4uBmiVPTkeE9l2K0WCypm4SG9QLo1ectgvcf5K/tu9ny106v7S+EmviTkybB2LFGwV0Nh+eePMi9AT/AtXNQrDPkrXvDududIV+OoknrrtRp/Dgz5ywCjH9b87bdqd+8E0uWr4nR5/Pho2nYsgvdn3udiIiIWMtSOiIyAfgTKCciIV6yc8/DWEAGYyZHLwI4BicfYjIJbAA+cBmheGATsNH5635sxEfjR58CXLoEcxNkhA/tj6nqZudzGLAbKISxuHEZsfyMiaQCbpY4qroWcFniNMexxHFMTF2WOBaLJQ2QkLBe7hgltyvONqowbTps3nydc2fNVOCe8vBSly3kvTwO0mc2y5NBZWId45U+z7B4zq8snPkzQ7/6EYAhX/3AewP6MXfaaD75/Ltb2p88dYYVq9axbP54KlUsx6y5SzyWpQZUtbOqFlDVDKpaWFVHR6svrqqnnc+qqn1UtZSqVlLVjW7txqhqaef4KY7rlVDVks7f6EdJX2T2WclFw6dp4o3GxhrnfmAdkM+ZsgIcB/I4SmdvAAAgAElEQVQ5nwsBh926hThlsZV7us5zIrJRRDZGRkbejogWiyWZSGhYLxejf57MF1+Ppvtzr6OqvNT/fZq37U7bJ3pz7nwoK1ato1GrF/j86xfIlXcVz78A3bsr7WotxP/MLAgsCaV6QqZcccrrsgq9cjWcivcYZbhz1z/Urnk/gYFZCAzMwoULN+NlbNq6g3oPPQBAo/q1Wbthi8cyS0xicVdwrxcRKRxXG1+sKz3hPf/8TSECgWnAK6p6IZpTp4qIz2N5FcqYuo4C40KQWONaLJakI3pYrxMnT8Voc/HSZRbP+YVJ0+YyY/ZCXn6+W4w2Pbt3JDIyih7dOjDn96UULVyArz9/n98X/cEPP02kQrkqnDgRwbuv/0iXzkBUOByeDmH/GP+3As14+T8f8vee4FvGHfbpu9xbodwtZS+/PohZcxfz6YdvABAVdf2Gw3q2oKycv3CBoKBAAEJDLxCU1XwOCspKaGiYxzKLR4aISDqM/90m4BTgD5QGGgKNMdtZIbENEKuSc4JhelIUAuT1RToRyYBRcP9T1elO8QkRKaCqx5zlSFe8nbgscRpEK1/uy/UtFkvqwBXW69thH/D9mPEx6t3Deu1zRSaJg7//2c/k6fNYtHQVkZFR1KxRhdWroWD+CrRpDUSch4MTIPw0FGoNOU3orRFD3/NJ3hFD3+Oj/3uN+s070+nxNqRLd/PH+4Wwi2QPuhmCKygoKyFHTwAQFnaRbNmyeiyzxERVOzi2HE9iHMILAFcw219zgcGqGqe1f1zLlY8DHTwcjwNeoz87uedGA7tV9Qu3qlmAy0KyOzc95BNsiWOxWFInLZrWo2rlilSvWsljvbewXmCWEaOum2wDZUuX4Mkn2rJo9i8s+G081Su9yomTULpUOrL6HYF9P5pIJsWfvKHgwMzQ3IMxN23zFDt27bnlOuHhxt4hwD8TQVlN+MR7K5Zj7fotXLp0mbCwizdmcQDV76/EytUbAFiy4k9qVq/iscziGVXdparvqGoDVS2nqlWcvcFfvSk4iGMmp6r7Eijbg5joKNvdnMffBj4BJjtWOQcxDn1gLHFaYSxxLuOEbFHVsyLissSBuC1xLBZLKiShYb0AHqhemWf7DGDn7r0M++RdXn3rI+o3686p01C1Undq1MjC2RNn4N+fwS8QinUB/9y3jOHLTO61AYP5Z+9+rl2L4LWXjXFh/5d70fPFN7ly5SoD33oZ4EbIrqpV7qVuneo0bNmFIoUL0Pf5bmTMmDFGmSUmItI+rnq3FcLYx4jtV5GILAMmY9LrHHUr9wPqYGZhq+KyjElObFgviyVlcqfCev39N0yeBDlzmggmhfzXwbEFkLmQcRHwy5yk109KRv/yG4881jUpwnpdVlWfovvfCUTEpV/yYvTOUue8IbBGVb3mH4rL8ORhoBcwQ0QKAWeBAMym32LgG3eTUIvFYvGFLJmzcD40jDy5cybpdf7aCkHZoHdvJcOZhXBsLQTdA0XaQ7r42twlP1FRUVy8dIXMmVOvkvYVVX0GQEQWAhVclvmOPcdYX8aIa7nyMjAcGC4imTCa9IrLB8JisVjiQ60H6zN15hKaNapJ5gD/JLlGVJTJJlChQhSnt8+DsL2QsypkrA8nU+9uR0RkJGs3bKNQ0VJkz549ucW5kxRxcz0DOAEU9aVjXNaV0TP0hbqXq+qFGJ0sFovFCw8++BB+fn78ueUvwq+GJ8k1Dh6AqTMv0ff6eObtCIECLSB3NiB1x5b3y+BH4cKladqs+S3GOHcBS0RkATfT8TyBWVH0Slx7cocxLgQCFATCnM+BwFFVLeKxYwrB7slZLHcvn713gHbZmlGm4GHkwfFQ5NHkFilVkNL25NwRkUe5mdf0D1Wd4Uu/uJYrizgDjwTmqeos57wNxgrSYrFYUh7ntvFMoRZk8ruKNF4MeR5MboksicMaTExkBdb72smXsF4PuhQcgKrOxrgHWCwWS8ri5B9cX1iX8GvpmHhmpVVwaQQR6YhRbI9j3M7WiYhPibt9MTE6JiJvAb86509iNv0sFosl2Tl4EDZuhMdqzoJVHQmLKkGd9xcwY4FPdgmW1ME7mCw4JwFEJA9mT26qt46+zOS6YMJtzXeOokDneItqsVgs8eDKFTh3LmZ5374we/hYrq9oDzkq8+bilVxNV5T777/zMlqSjHQuBedwBh8TDHidyTkuA32cNOeqqlfiJ6PFYrHED1Xj0L13L+zZA37Om+vcOSh7/QuG9O7Pou1NCa85nelzA2nWDNLFN8eKJSXyuwfrynm+dPT6NRCRiiKyAfgH2Csi65yAmRaLxXJH+O03WLDAZO+eP98pVOXQ7LcZ0rk/p/wf571ls3m0QyCnTkELm3EyTaGq/wG+B+5zjlGq+qYvfWN1IbjRQGQVMEhVFznnTYD3VfWhBEmdxFgXAoslbXDlClSoAFmymJlb5cowb04UbHwRgkcxYcNzdBr6LSdOpadWLbNHd/w45MvnfWzLraRwF4J8wAM41pXRli9jxZcJfVaXggNQ1cWAzQthsVjuCEOHwoEDMHw49OoFSxZd4+KiLhA8io9nDWBX4EgkfXry54c//jAzPavg0hZJbV15QEQGAL84512BA/GQ02KxWG6LQ4fg44/h8cehUSMoW/Iyta4+TuCZ+ay+PIS3J73Ojh032xctag5LmiPe1pW+KLkewIfc3ORb6ZRZLBZLkjJggPk7dCgQcYHCe1tT8L5VvDZxFKuOP0ulSlDRa3ZLSxogSa0rzwAvWutKi8VyJ9m1CyZMgDfegGL5z8CS5nDuL7YFTmDY7CcAGJywFHSW1EO8rSt9MTypiElpUMApOgI8o6q74iXqHcIanlgsqZvOnWH2bDi45xi5tjaFsGCoO43rBR6mVCmzT7dvH5QsmdySph1SuOHJY9yMtrUywbEr3fgeeDuadeUoIEVbV1osltTLrl0waRL8991D5NrUGK4eg4bzIV9D0gFDhsD69VbB3U2o6jRg2u3282Um95eqVvZWltKwMzmLJfXSuTPsWLufrZ83JH1UKDSYD3lqJ7dYaZ6UOpMTkfbAp5i8puIcqqrRU8LFwFpXWiyWZOHqVciYMWZkkl27YNPyvWz4pCHpr1+BxktNwlPL3cxnQBtV3X27HX2xTumBiV05zzmK4IN1pYiMEZGTIrLDrWySiGx1jgMistUpLy4iV9zqRrr1qSYi20UkWESGy12WKdBiSas0aQIPPWScvV1ERcGXg3azYmB9smYOh8bLrIJLQcTyXh8iIn+LyDYRmSEi2d3qBjjv7j0i0tytvIVTFuwkAPDGifgoOPBhuTK+iEg94CIwTlXv9VD/ORCqqh+ISHFgTizt1gN9gXUYJTtcVedHbxcdu1xpsaRc9uyB8uXN506dYPx483nwGzt5tmQjsmYVMrdeAtmtf8CdxNtypaf3uog0A5aqaqSIfAqgqm864R8nYKKUFMT4tZV1hvoHaAqEABuAzp6MGZ1lSoD6QH7gN+BGOnlVne7tnrwuV4pIaeA1oLh7e1VtFlc/Vf3DUV6exhSM13ojL9cuAASp6lrnfBzQDpMNwWKxpFKmOeYDffuaSCYVK0KhLNvoXaoxGQMykLnNUshWPnmFtMTA03tdVRe6na7FRCUBaAtMVNVw4F8RCcYoPIBgVd0PICITnbaeLPbbuH2+DLjrHQUSruQwHuWjMfnkonxo7wt1MdPPvW5lJURkC3ABeFdVVwKFMJreRYhT5hEReQ54DiBjxoyJJKrFYklspk6FWrXgyy9NPMqpP25l8YAmiJ8/Wdstg2xlklvEuxU/Ednodj5KVUfdRv8ewCTncyGM0nPh/v4+HK28pqfBVPWZ27i2R3xRctdVdURCLxSNztx06gM4BhRV1TMiUg34zfHPuy2cf4xRYJYrE0VSi8WSqOzfD1u2mCgmIvDDp5sJ/70JVyMDydZuGemylUpuEe9mIlW1enw6isg7QCTwv8QSRkTeUNXPRGQEZuZ2C6ra19sYsSo5EXGZZs50ZkgzuHUt9MLtiwwi4ge0B6q5jRXuGltVN4nIPsza7RGgsFv3wk6ZxWJJpbiWKh97DDizkUyrmpIxRxBBTZZBoHV8S42IyNNAa6Cx3jT0OIIxVHTh/v6OrTw6LmOTjbHUeyWumdxOjOZ0WTMOdKtTTIbw+NAE+FtVbyxDOsE2z6pqlIiUBMoA+1X1rIhcEJFaGMOTbkBizyotFssdZOpUqFYNimfdAEubQsYcSONlEFg8uUWzxAMRaQG8AdRX1ctuVbOA8SLyBcbwpAwmk4AAZUSkBEa5dQK6eBpbVWc7f3+Or3yxKjlVLRJbnS+IyASgAZBbREKA91R1NOaGJkRrXg/4QEQigOvA86p61ql7ERNWLABjcGKNTiyWVMrBgyZSyc+f/wlLW0DGXNBkGWQpltyiWXzA03sdGABkAhY5Hl5rVfV5Vd0pIpMxBiWRQB9VjXLGeQlYAKQHxqjqzliuNxsPy5QuVPURrzLH5kIgIvVVdYWIeBxEVWd5Gzw5sS4EFkvKY9gwmDZyFX8Makm6zPmNo3eWBP2etiQiKS3iiYjUj6teVVd4GyOu5cqmwAqgg6exMVNRi8Vi8Yldu2Dz/OUsGvAw6QKLQKOlkLlgcotlScG4KzERCcAYKO65nTGSzBk8ubEzOYsl+Rg9Gk6fhvvugzJlYORI2Pz7Cua83pIo/xJkbbsEAvInt5iWaKS0mZwLEWkDDAUyqmoJEakCfJDQ5co4TTNVdXh8hL1TWCVnsSQPJ09C/vzg/mqpW34lCwe0xC9bUfyaLYOAfMknoCVWUrCS24QJHrJcVe93yraraiVvfeNarsyTSPJZLJa7iLlzjYJbuhT8/ODM36tpE9iK9FkLmz04q+Ast0+EqoZGC13s0zJkXNaVA2Ors1gsltiYMwcKFYIGDUDOrINjLSGggNmDs0uUlvixU0S6AOlFpAwmnvEaXzp6zUIgIqVFZIGI/OWc3+ek3rFYLJZbCA+HhQuhdWuQsxtgWTPwz2tmcNbIxBJ/XgYqYoKGjMeEf3zFl46+pNr5ERiE8V8D2I7JKWexWO5iVGHzZqPYXCxfDhcvwpOtNsPSZpAxp6PgCsc6jsXiA/lU9R1VreEc7wBe9+PANyWXRVVvTAudkC0R8RTUYrGkcq5cgR9/NJaT1arB88/frJszB2qV3cJD4U0hQ5DJB5clvsGRLJYbTBORG8H5nZQ/Y3zp6IuSO+OEX1Fn8HbA8fhIabFYUjeHDkHp0vDss8aopF07GDvWzOBUYf+mzSx8qzGSIdBEMrGhuiyJQ29M4P78ItIKE96xlS8dvfrJOfnkRgG1gFOYjAGdVfXfBImcxFgXAoslcYmIMMYk27fDjBnQqJGZ1VWqZBTeb2M2kX9XE/wCspl0OYElkltky22SUl0IAESkNvA9cBV4WFVP+dLPl1Q7h1S1kYhkwyjF8+7pzS0Wy93B++/DmjUwYQI0bmzKMmeGb7+FgS+up+DfzTl7KTuZGy0jq53BWRIBD7ErMwOhwGgRSVjsymgXaecWWDMvMFdVa8Rb8juAnclZLInH4sXQrBn07Ak//BCt8tRqLs9rybGzeeg3aylzltlgy6mVlDaTS+rYlS7mAZNFpAMm789s4C2fJLRYLCmaAwegd2+zt9ajB2TKFLNNaCh07Qr33ANffRWt8sRyWNGajNkK0X7wUrr1LhRzAIslnviixLzhU+xKEekHNARKAS+q6sqEXjipsTM5i8U7vXqZOJMAhQvDgAHGWjKdm0na55/D66+bFDk13Ndvji+BFW3M3lujJUT45cfPz2T7tqROUuBMbpWqPiQiYdy6bCkYY/+gWLrebOhj7EoBngG24WRotbErLZbUzeHDUKqUsZRs1w4GDYLVq+HLL6FfP9MmMtJYUxYrBivcf1MfWwh/tIXA0tB4iXH4tqR6UpqSSwziciHI43bkxixTHnQrs1gsqZihQ43Z/xtvQNOmsHKl+TtoEJx1UhbPnGkSnb76qlvHo7/Dikcga1njB2cVnCUZEJFDPrWzqXYslruPkyeheHHo2NH4ubnYvh2qVIG+fU2C07p14cgR2LsX0qcHjsyBlY9BtgrQaDFkypVMd2BJClLTTE5EDquq14y7sRqeiMjnqtpfRGbgIdqzqrZPoIwWiyWZ+OoruHoV3opmQlapkrGg/PpreOABWLXKKLv06YFD02B1J8hRBRougEw5k0V2i8XBpxlaXHtyD6jqehFp7HF01SUJEC7JsTM5i8UzoaFQtKhxCZgyJWb98eMm0emVK8YPLiQEgs6Ohz+7Qa6a0GAeZMx25wW3JDkpbSYnIq/FVgW8o6pef2nFuienquudv0uiH0APH4QbIyInRWSHW9n7InJERLY6Ryu3ugEiEiwie0SkuVt5C6csWESs64LFkkBmzIALF4zFpCfy5zdWllFRxq0g6PTPsKYr5KlrZnBWwVnuHFljOQKB6A4tHvHFT84TdX1oMxb4GhgXrXyYqg51LxCRCkAnTCqFgsBiESnrVH8DNAVCgA0iMktVd8VTbovlrmfZMsiVK5o7QDRefdVkF3it3U+wtifkbwz1ZoJf5jsnqOWuR1UHJXQMXwI0xwtV/QM462PztsBEVQ13YmIGAw84R7Cq7lfVa8BEp63FYokHqiaYcoMGt/rCRScgAAZ1G0223T0hfxOoN8sqOEuqJC7Dk/tiqwIyJOCaL4lIN4y/XX9VPQcUAta6tQlxygAORyuvGdvAIvIc8BxAxowZEyCixZI2+fdfk0ngjTe8NAweBet7Q4HmUHcG+AXcEfkslsQmruXKb+KoC47n9b4DPsRYxXwIfI4P+3u+oqqjMBkTyJIlS9r0jbBYEsCyZeZvw4ZxNNr7HWx4EQq0hHrTIb3/HZHNYkkKYlVyqurLvtttoaonXJ9F5AdgjnN6BHD3dyjslBFHucViuU2WLYO8eU0cSo/sGQGb+kLB1lB3KqT3EMzSYrnDiEg+4L9AQVVt6dhx1FbV0d76JtmenCdEpIDb6aOAy/JyFtBJRDI5CVrLAOuBDUAZESkhIhkxximz7qTMFktaQdUouQYNYokvuXuoUXCF20LdaVbBWWIQi9V8ThFZJCJ7nb85nHIRkeGOZfw2Eanq1qe7036viHT34dJjgQUYw0SAf4BXfJE5yZSciEwA/gTKiUiIiPQEPhOR7SKyDRPw+VUAVd0JTAZ2Ab8DfVQ1SlUjgZcwN7cbmOy0tVgst0lwMBw96mGpUhW2fwBb/gNFO8CDkyG93dO2eGQs0CJa2VvAElUtAyzhZpaalpgJSxmMrcR3YJQi8B7GvuIB4D2XYoyD3Ko6GbgO4OiGKF8Ejq8LgVdUtbOH4linlqo6GBjsoXweJt2PxWJJAB7341Thr7dh1ydQohvUHA3pkuy1YEnlqOofIlI8WnFboIHz+WdgOfCmUz5OTcSRtSKS3VnNawAsUtWzACKyCKM4J8Rx6UsikgsnyomI1MIkT/WKT99mEekElFLVwSJSBMirqpt86WuxWFIGy5ZBgQJQ1uWBej0KNr4EwSOhdG+o8S3IHd3BsKQ8/ERko9v5KMegLy7yqeox5/NxIJ/zuRAxreMLxVEeF/0xW1WlRGQ1JknA4176AD4oORH5GuMyUA8z07oEjARSdGZwi8VyE9d+XOPGzn5cVLiJYnJ4KlR4Eyp/bBPBWQAiVbV6fDurqopIolu2q+omJ0t4OYwb2x5VjfClry8zuTqqWlVEtjgXO+sYgVgslhSIKpw4AX//bdLkXLsGp0+bsoYNgYgL8MejcGIp3P853BNbeECLxSdOiEgBVT3mLEeedMpjs5o/ws3lTVf58rgu4NhxTAQmqeq+2xHOFyUXISLpuLkWmgtn889isaQcoqLg449NJu/z52PWZ8oEzeufgMWt4PxfUHsclHjqzgtqSWvMAroDnzh/Z7qVvyQiEzFGJqGOIlwA/NfN2KQZMMDLNdoATwCTReQ6MAljiOg1p5zXfHJOdJJHgerAGKAjMEhVJ3obPDlJ7CwES5aY/YwKFRJtSIsl0Th4ELp2NalxHnkEmjSB8uWhZEnw9wc/P8gq/5J5bTO4cgQemgqFWnkf2HJX4S0LgWM13wCTSPsExkryN4x1fFFMYu2OzoqfYOIXtwAuA8+o6kZnnB7A286wg1X1p9uQsQwwEHhSVdN7be9L0lQRqQg0wayFLlbVHV66JDuJqeTGjDE5trJkMRHcmzZNlGEtlkRh1Spo3RquX4dvvjHKLsb22rltsKw5XA+H+nMhT+1kkdWSsklpqXbcEZFimNncExj3gUmq+rnXfj7M5L7ABE9enxiC3iniq+T69YNq1aBDBxOk9uef4ZlnzC/jEydg92745Rd44gnT/tw5yJYt7mC3FktSEREBlSubBKiLF5uZWwxOrYHlD4NfFmi00GT1tlg8kFKVnIiswxhATsEot/0+9/VByfXEaM4SwDSMwtsaf3HvDPFRcmFhJv3Inj2QIwe0bAkTJhgFN3OmST3yyCPml3P16rBvH5w9C3XqwNKlZs/DYrmTfPMNvPQS/PYbtPWUn+PYQmNkElAQGi+GLMXuuIyW1EMKVnLlVHVPvPr6slzpXMTll/AEkF9Vy8fngneK+M7kXKlIvv8epk+HunVh9myTIRlMtuRXXoF//oFy5SAw0Gz0v/wyDB+euPdgscTF+fNQujRUqmR+ZMVYojw0BdY8CUEVTLLTgHwex7FYXKQ0JSciXVX119gyhKvqF97GuJ3QBkWA4hinvfhmIUjxiBgz64YNTfbkzJnNpr2LgACjAN2JioIvv4R69eBxn9wTLZaE89FHZiVh2DAPCm7PCNjUD/LUgfqzIaO3qEkWS4rEpXCzeqjzaYbmy3Llf4HHMB7qE4EZqnrmNoRMFhLbujIurl0zM76//zbLmpcumbxdFSt6SWliscST4GBj6fvUUzDaPVieKvz1Duz62ARarjPB5oKz+ExKm8m5EJEHVXW1tzKPfX1Qcn2Aqe5pclIDd1LJgTHhvv9+Y4jiIijIJKjMlu2OiWG5Czh3zmQS2L/fLJsXcOX2uB4B65+D/WOh9HNQ/Rsbh9JyW6RgJbdZVat6K/NEXJnBy6jqXmAlkM/J53MDVd0WX4FTLKqwuD5kr2ScZHPV9DnUUbFisG4dbNwIxYub2VzTpvDdd/DWW167Wyw+cfEitGplVg3mznVTcJGXYVVHODoX7n0PKr1nw3RZUj0iUhuoA+SJti8XBHj1kYM4ZnIiMlpVe4rISg/Vqqr1blfgO0m8ZnIRF2FdLzgyE6KuQtYykKOqMb32C4ScVaFIe8jgaXk4Ji1awJYtcOCA2cuzWBLC1avw8MOwYgVMnQrt2jkV4WdgeWs4u97M3so8n6xyWlIvKW0m58SrbAA8j4mZ7CIMmO1MxOIew4flygzRA2F6KktpJGi58looHJ4GByfCpYMQecnE+4sMg/QBUPhRKNMb8sat51esMMtKX38NffrETxSLxUWfPvDtt8ZPs2tXp/ByCCxtBhf3w4PjzY8wiyWepDQl50JEiqnqwXj19UHJxXstNDlJ9D05VTi9Fv4dB4cmwbVzkLcBVHof8tWPtctDD8GRI7B3L2TIkHjiWO4udu6E++4ziu6Gq8qFf2BpU/NdrD871u+hxeIrKVjJ5QHeACoC/q5yVW3krW+scTpEJK+IVAYCRKSSiNznHA8BmRNB7tSFiAmF9MB30O4IVP0SLvwNSxqYX9Khf3vsMmCAMUqZEFc6QIvFjcOHYVO0bI3/+Y8xZHrvPafg7CZY9BBcvwpNVlgFZ0nr/A/4GxOUZBBwANjgS8e49uSeAXoAVQD3CCdhwE+qOiX+8iY9d8S6MvKKSTi5fRBEXYbyr0HFdyFD4I0mqlClCpw5A7//Dvfem7QiWVI3V6+aMF3BwWZpsndvWLQImjWDoUOhf38gZBas7gz+eaHhQggqk9xiW9IIKXgmt0lVq4nINlW9zynboKpe85r6slzZUVUnJ5Ksd4w76kJw9SRsfQv2/2TCJtX+BfLWvVG9ZYsxGAgLg4kTzWeLxRPvvguDB5vwchs2mHOX7+WuXZDpwHDY9ArkrG6WKG0UE0sikoKV3FpVreWk6RkOHMW4tpXy2tfHLATNibkW+l8vfcYArYGTqnqvUzYEkxfoGrAPk3rhvIgUB3YDrthka1X1eadPNWAsEADMA/qpD0LfaT85AE6thj+7GyOACm9ApUGQ3gS0DAkxcS//+gs+/BBefdVaXFpuZds2Exz8ySfhxx/NLG7MGFM3edJ1OpR6DfZ8ZQyf6vwKfnffroElaUnBSq41xp2tCDAC40IwSFVnee3rw0zuWyA7UA/4CRP9ZK2q9vDSrx5wERjnpuSaAUtVNVJEPgVQ1TcdJTfH1S7aOOuBvsA6jJIbrqrzvd1Ysig5MG4IW/pD8CjIcT/UnQqBJjT8pUsmo8GUKZA/v9ln6d3bpPCx3N1ERZlA3//+azJd5Mpllro//RQOHbjKN092Qw5PgXKvwP1DIZ1PLkIWy22RUpVcQvBFyW1T1ftE5C9VrSwiWYG5vvjJeVFejwKPq+qTsbVzUqkvcwWDFpHOQANV7e3t2smm5FyEzDKzOoA6v0Ch1jeqVqyADz4wQXXLljWzO3//WMax3BUMHmyWJsePh86d3SquhcIf7eDkcqPc7umfXCJa7gJSqpITEU/h70OBjao600PdDXzJgnbF+XtVRPIDV4GCtyeiR3oA7jOyEiKyRURWiIhrQ6sQEOLWJsQp84iIPCciG0VkY2RkZCKImAAKPwItN0FgcVjRBrb9H+h1AOrXN5nGp00zYZlGjox7KEvaJTwcnn3WKLgOHaBTJ7fKy0dhcV04tQpq//r/7Z1nmBTF1oDfwy6w5JxzRlAkBwkSV1EEQRQQA4oiiigoV4yAAQNeEFREQZJKFEX5uJIkRyUKSFAEkQySMyx7vlUdNEcAACAASURBVB9VA8OyYTbO7Gy9z9PPdFd3VZ+ehj1zqk5wCs6RlgnDOEH+abcqQFGgq4gMjbWnqsa6AQMw05X3AweBfcB7cfWzfUsCm6Npfw2YzjVLMiOQx+7XwCSDzg7UxFQi9/RriLH44rx35syZNSC4fE51ZRfVCaguaa96+ex1p5s3V82bV/XUKT/J5/Abe/ao1q6tCqqvvqoaEeF18sRW1enFVadkVd0/128yOtIWwFn14e9rSm/AKiDE6zgUWIlJ7bUltr5xWnKqOkBVT6gJGSgF3KKqr8TVLyZEpAvGIaWz/VJR1YtqKxuo6lqMU0p5q1CLenUvattSD6GZoM4YM9W05zuTG/P8gaun330X/v3XlEtxpB0mTzbB3Vu3mrqFAwdCiGeZ7d9VMK/+tRi4Qi38KqvDEQDkArJ6HWcBcqvqFeBibB3jVHIi0tqzAS2ABiJyu4jkia+UInInJmq9taqe82rPJyIhdr80UA7YqaoHgFMiUldEBHgEiHX+NSARMVNNjX6AU1thTp2rweO1akHbtiYG6t9//SynI9k5ehQ6dDDrbuXLm4Tebdt6XbB/FsxvZuq/tVhh8qU6HI5BwAYRGSsi44D1wIcikgX4ObaOvjiezALqAYttUyNgHVAC6KeqE2PoNwmTWDMvcAjoD7yCmZr01KNbpardReQ+4C3gMhAJ9FfV/7Pj1ORaCMEsoKfGJTQB4HgSE8fWw6KWoBHQeDbkqcmWLaa68zPPwJAhLv1XsLJ2rVFoBw/CgAHw0kvXF+Rl1wRY1cVUwWg8y8XAOVKcQHU8gauOiLXt4WpV3e9TPx+U3Bygi7WqPDcaAzwMLNJoPCcDgYBVcgCnd5icgxf/NdZdwWY8/jiMHWu8LGvWhJYtoW9fryksR6pmwgR44gnIn99MT9aoEeWCbcNgXS8o0MT8m0if3S9yOtI2gark7ExeZ6C0qr4lIsWBgqr6a1x9ffGuLOZRcAB2v4Sq/gv42YUxlZKtLLRYDllKwqK7Yf8sPv8cpk6Fp5+Gy5fhtddMMl4fYvUdAc7AgaZqQO3aZnryOgXnqeS9rpcJ8m78k1NwDseNfIaZUfQE2JwGhvvS0RdL7nOgEOBJ7XU/ZvrxBWCWBmhduYC25DxcPGYsupOboeF0KHLX1VOvvgrvvWcKrr73nh9ldCSKM2cgXz5TW3Dq1ChT0ZFXYPXT8NcoKPMk1BrhgrwdfiWALbl1qlpdRNarajXb9puq3hpX3xgrg3vxDEaxNbDHk4GpqhqJWZ9zJJSMuaHpPFgYDkvbQsPvrgaNDxwIx4/D++9Drlxm/caR+pg50yRd7t07ioK7cglWdIY906Dya1DlbVfJ2+GImcvWOVHhaumdSF86+pq7sihQTlUXikgYJl4hoM2kVGHJebh03JTrOfEbNPoRCrcETKqnzp1hyhST1/CWW/wspyPetGsHq1aZ8jlX11evXIRlD8C+GVB9CFTs7VcZHQ4PAWzJdQY6ANWB8UB74HX1oRqOLyEEjwMzgC9tU3FSoxt/IJMhl7HoctwCS9rCwfmA+aP42Wcmt+WgQX6W0RFvTp+Gn36C9u29FFzEefOO982AmsOdgnOkOkSkt4j8LiKbRWSSiISJSCkR+UVEdojIFBHJYK/NaI932PMlE3JPVZ2ACT97DzgA3OuLggPfHE+eA+oCp+zN/gDyJ0RQRyxkyAlN50K2ciYN2OElAOTObZI4T5oEf//tXxEd8eP//s+k7XrgAdtw5QIsaQMHZkPtUVD+Gb/K53DEFxEpgtEJNa1nfQjQEfgA+EhVywLHga62S1fguG3/yF6XIFR1m6oOV9VPVXWrr/18UXIXVPWS58DOi7rFg+QgYx5o+jNkKW68Lo+uAcx6Trp0MHiwn+VzxItvv4XChU11Aa5chCXt4ODPUHcMlH3C3+I5HAklFMgkIqFAZoxl1RSYZs+PB+61+23sMfZ8MxsO4BMiclpETtnNe/+ciPjk3e+LklsuIi8BYSLSBJgCzPRVSEc8yVQAms43Cm9RSzi1naJFjQv66NFw5Ii/BXT4wqlTMGuWSbqcTi/BsvvhwCyoPRJKd/G3eA5HTIR6ktzbrZv3SVXdB/wX+Aej3E4Ca4ETqupROt6J9ItgchFjz58EfM6WparZVDW73bJhigMMxORRHubLGL4ouZcwMQnbgOeB+ZgEy47kInMRaDIPEBNicHYP//mP8dL75BN/C+eIDlXo1QuefdbEws2YYacq74+AFQ/Cvv+DWp85C84R6ESoak2vbaT3SRHJhbHOSmEUThbgzuQWSkRyisgAYCOQDailqj6V5fDJuzI1kqq8K2Pi2HqY3xgyFYEWy2jXKTcLF8KiRXBrnNEhjpRk4UJo2tREAahCxoyQL5/yz9QnkF1jnBelI1UQl3eliNwP3KmqXe3xI5gg7fsxGUgiRKQeMEBV77AZswao6ko7vXkQyOdLakY7fl7gRYxn5RjgE1U9Ga9niuleIjIPG5MQDaqqd8TnRilNUCg5gEOLTRxdntpsLTSPZuFhHDtmEjr36OFCqwKFJk1g+3ZYtw5++AEmTlSGPvoS1TP9F27uB1Xe9LeIDkec+KDk6mCUTS1MrdFxwBpMzPR3qjrZJhDZqKqfiUgPTOWa7iLSEWinqg/EMHx09zsLHAHGYmYUr0NVh8Q5RixKrk40zTUx05fHPFHngUrQKDmA3VNheQco1p4jFabQ5bF0/PQTtG4NX3wBBQv6W8C0zZIlphDu0KHw/PO2ccsHsOFlKP8s1PjY/RpxpAp8iZMTkTcxllUEphrAE5i1t8lAbtv2kKpetHHVXwPVgGNAR1XdGQ95BhCzsYWqxvnr0ddg8NuAfkAO4F1PhYBAJqiUHMDWIbD+RajQi8hqHzFsmEn5lTkzfPghdO3q/o76ixYtYNMm2LnTvA92fQ0rH4ESneC2b0B8Wfp2OPxPoAaDJ4ZY//eJSDMRWYTxZhmsqvVSg4ILSir2hvLPwfahpPtrBL17mywoVarAk0+a9aAzZ/wtZNpjxQr4+Wf4z3+sgjs4H1Y9DgWaQt1xTsE5HH4mtunKVUBB4ENgadTzqroxeUVLHEFnyYFJ6OsJJm4yFwo2JTISRo401Qs+/BD69PG3kGmHCxdMSaTNm02gfpbLm2BeA8hcHFosgww5/C2iwxEvgtGSi03JLePaXKhyfQC4Bmr1AQ9BqeQALp+CubfB+f0Q/gtkLwdAs2awbRvs2gUZMvhZxjTA77/Dgw8aa3rECOj+8B6YW8+cDF8JWYr5V0CHIwGkKSWX2glaJQdwZifMqQ0Z8xpFlyEHc+aYci5jx0KXLv4WMLgZMQJeeAGyZzff913NjhkL7vw+aL4UclXxt4gOR4IIVCUnIhmB+4CSeFXPUdW34urrFgxSI1lLQ4Pv4PRfsOIh0EjCw8363IcfQqRPBSgc8UUV+vWDZ54xIQMbN8Jd4edhSWs485epIOEUnMORHPyICUKPAM56bXHiLLnUzB+fwZoecPMbUOUtvvkGHn7Y1DC7+25/CxdcqBpv1kGDjCfrF19AiFyBZe1h74/QYAoUv9/fYjociSKALbnNNiF0vElWS05ExojIYRHZ7NWWW0Tmicif9jOXbRcR+diWZNgoItW9+jxqr/9TRB5NTplTFeWehjJdYfPbsOd7OnSAYsVcWZ6k5vBhY70NGmQcfEaOhJB0Cmufh70/QI2hTsE5HMnLChFJUEVNX+rJVYlmKyHik2/0OG7Ma/YyMF9Vy2HyYL5s21sC5ezWDRhh758b6A/UAWoD/T2KMc0jYmqS5akLKx8h/bktvPCCCU6eN8/fwqV+Zs2CNm2gSBH4/HOzDjd8uKkIwbYh8OdwqPgiVHjO36I6HMFOA2CtiGy3RtAmEfHJwz/O6UoRWQ1UBX7HeFjeBGzBJMnspqrz4+hfEpjpMTVFZDvQWFUPiEghYJGqVhCRL+z+JO/rPJuqPmXbr7suJtLEdKWHc/thdnXIkJPzjVZTvU42Tp82Acq53M+BBPHxxyZ7SYECZgr4scegUiV78p9vTWXv4vdD/ckuFs4RNATwdGWJ6NpVdXdcfX353/k3UENVq6rqrUAN4A/gDiAhFc4KqOoBu38QKGD3r5ZksHjKNcTU7vCQuTDUnwKnd5Dpt8f55mvl0CEztRakS67JytixRsG1bQt79hhnnqsK7shyWPEw5KsP9b5yCs7hSAGsMssJ3GO3nL4oOPBNyd3kHfitqpuASqq6IyHCemMzUSfZn2ER6eapgxQR4VM9veChwO1w63uwZxo1sg7lzTdhyhSYONHfgqUuvv0WnngCwsNNNfb06b1OnvoDFreGLCWMJ2VImN/kdDjSEiLyPDAByG+3b0Skpy99fVFy20TkExGpb7ePbVtGjDtnfDlkpymxn4dt+z7AO4K2qG2Lqf0GVHWkpw5SaGhodJcENzf1gWLtYP1/6Pv4MurXNw4TOxL9cyRt8Pff0LmzqeT9/femXM5VLhwxRWwlBBr/ZIraOhyOlKIrUEdV+6lqP6Au8KQvHX1Rco9gpghfttt+4FGMgmuWAGFn2P7Yzx+92h+xXpZ1gZN2WnMOEC4iuazDSbhtc0RFBOqOhSylCFnZgQljDpM+vcmG8s8//hYu8Bk/HiIiYMIEyOK9KhFxzlhw5/fD7TMgWxm/yehwpFEEuOJ1fIXrs3DF3DE54+REZBLGcSQvcAjjJfkDMBUoDuwGHlDVYyIiwKcYb8xzwGOqusaO8zjwqh12oKqOjeveacrxJCrHf4O5dSFfA9bnnE2TpiHkzWu8LgsX9rdwgUlkJJQtC2XKRPFMjYyApfeZyt4Nv4Nibf0mo8OR3ASw48kLGKNoum26FxinqkPj7OuDd2VdjHIqwfXpVMonVOCUIE0rOYC/RsMvT8DN/Vl1bgAtWkDRorB0KeTN62/hAo/Fi6FxY/jmGzNlCRivndXdYcdIqPEJVHjWnyI6HMlOoCo5ABs73cAeLlXV9T7180HJbcUUSl2Ll7moqocSJmrKkOaVnCqsegx2fQVN5rD4jxaEhxuHihkzXO25qDz2GHz3HRw8aEvmAGx+Bza+AZVehqrv+VU+hyMlCDQlJyLZVfWUjZe+AVU9FucYPii5X1Q1uirhAU2aV3Jg1pLm1IaLR6DlBoaNLESvXvDJJ/CsM0qucuaMqa7eqROMGmUbd30DKx+Gkg9DvfHuV4EjTRCASm6mqrYSkV1c74kvGAf90nGO4YOS8/yE/R646Gl39eRSCSe3wOxakKc22uRnWrUOYf58+PVXk9DZYRxOunSBZcugfn3g8BJY0BzyNYDGsyHE1S5ypA0CTcklBb4ouRsKpuLqyaUudo6HVV3g5n4cLvgmVapAnjywerXX1Fwa48wZM6MbFmamcPfuhT/+ADn9h6kLF5bP1IXL4FLGONIOgarkRGS+qjaLqy064gwmU9WGiRHOEQCUfhQOL4LNb5M/f0O+/ro54eEm6HnChLQ3E7diBTRqBFe8HJLfeQfk0lFYdPe1WDin4BwOvyIiYUBmIK8NIfP8tcqOj5mvYlRyItJJVSeJSLTZZ1X143jK6/AnNT+Fo6th+YO0aLmegQOL8NprULkyvPaav4VLOSIjoXdvk5Oyd2+4eNFYdD26X4QlbeHcHmi2wNTsczgc/uYpoBdQGOP86FFypzAhZ3ESmyXn+RmbL6HSOQKI0CzQYBrMqQnLO/BK34Vs2ZKe11+Hm26Cdu38LWDKMHmyWY8cNw4e9aQkUIVV3eDIUrhtIuS7zZ8iOhwOi6oOA4aJSE9V/SQhY7iiqWmNvyfDik5Q8UUuVPrv1QrXK1cGvyPK+fNQsaKJE1y92pbMAdg8EDa+Dre8Bbe84VcZHQ5/EqhrcgAicjNQCbiaNFZVv4qrX5xrciKSF3gcKMn1weDdEiKow8+U7AhHlsG2wYTla8D06fdStSo8/jisWgXBnPJz6FCT3mz8eC8F9880o+BKPgQ3v+5X+RwOR/SISH9M9qxKwE+Y+qPLgDiVnC/elcuBVdwYDD4lwRKnAM6Si4UrF2FeAzi9A1quY+pPpejQAQYPNoVBg5FDh6BcOWjaFH74wTYeWw/z6kOuqtBsIYRkjHUMhyPYCVRLTkQ2AbcC61X1VhEpAHyjqi3i7OuDktugqlWTRtSUwym5ODizE2ZVh2zl0ebLuOfeDCxcCL//DiVL+lu4pOfBB01Gk02boHx54PwhmFMLULhjNWQq6G8RHQ6/44uSE5GcwJfAzZgA7ceB7cAUzIzf35icxMdtTuJhwF2YnMRdVHVdAuT6VVVri8haoAlwGtiqqhXj6utLFYJZIhIeX6EcAU7W0qZiwbHVyG8v8dlnJpQgGAutzpplasO99ppVcFcuwtK2cPFfaDTDKTiHI34MA2ZbBXMrsBVToWa+qpYD5ttjMNOK5ezWDRiRwHuuscp1FGZWcR2w0peOvlhyx4EcGC18iWvpVKLNJRYoOEvOR9b2gu3DoOF3DPu+Hb16mWKrDzzgb8GShrNnTZhE5sywfj1kzKAmMH7XV9BgKhS/398iOhwBQ1yWnIjkADYApdVLeYjIdqCxqh6wdUIXqWoFEfnC7k+Kel0iZCwJZPc165YvllxeID1G0eWzxy6sIFioOghy14JVj/Nsl53ccouxeC5f9rdgSUP//rB7N4wcaYugbvnAKLhb3nQKzuG4kVARWeO1RXUwLAUcAcaKyHoR+VJEsgAFvBTXQaCA3S8C7PHqvxcfg7jBVB6IugG5rZzVfXqgWAYvp6p/ApVjuCSgc1c6fCQkAzSYArOqE7LyAd57Zzmt2mRk3Dh40qe6u4HL7Nnw0Ufw1FPQoAGwZzr89gqU6Ag3u1ABhyMaIlS1ZiznQ4HqQE9V/UVEhnFtahIw03wiklSLHoPtZxhQE/gNM5tYBVgD1ItrgNgsOY/gw6PZfIo0d6QSspay63NruatQH+rVgzffNHFlqZXRo6FVK7jlFnj/feD4BljxEOSpA3XGpL1cZg5H0rAX2Kuqv9jjaRild8hOU2I/D9vz+4BiXv2L2jafUNUmqtoEOABUV9WaqloDqObrODEqOVXtaj8bRrMFdHJmRwIodi9U6I38+Smj3pjGvn0wIqFLxH5EFV5/3eTlbN7cFInNGXYEFrcxuSgbTYfQTP4W0+FIlajqQWCPiFSwTc2ALcAMTOVu7OePdn8G8IgY6gInE7geV0FVN3nJsRm4yZeOPmU8EZGK3BhpPjH+cqYczvEkAVy5BD83gpNb6DJlHTMXl2XnTsie3d+C+UZkJPToAZ9/bpTcZ59B+nSXYGELOPortFgGuWv4W0yHI2DxMYSgKiaEIAOwE3gMYzBNBYoDuzEhBMdsCMGnwJ0Y58XHVHVNAuSaBJwFvrFNnYGsqtopzr4+eFe+DoQDFYE5wB3AMlUN6GyHTsklkLO7YVY1zkoJ8nRaSa06YXz0EdSMbZY+AIiMNGtvX34JffvCe+/ZGclfn4Ydn5uclCXj/P/gcKRpAjgYPAx4GvDMIi4BRqjqhbj6+uJd2QETfHdAVR/GxEUk+EsQkQoissFrOyUivURkgIjs82q/y6vPKyKyQ0S2i8gdCb23wweylIB6X5Hl4gY2fNmLbdugVi0TTH3woL+Fi57ISOMk8+WXxjP0qoL7c4RRcJX6OgXncKRiVPWCqn6kqm3t9pEvCg58yF0JnFfVKyISISLZMO6hJRIh7HagKoCIhGAWD6djTN6PVPW/3teLSCWgI8bLszDws4iUV9UrOJKHIq3gppeouHUQu5c0YuA3DzJ4MJw6BTNn+lu4GxkxAsaMgX79YMAAq+AOLYQ1z0Hhu6HKQH+L6HA4EoCITFXVB2xarxumHVU1zrTyvii59TbSfAzGZfMU8Gt8hY2BZsBfqrpbYvZ2awNMVtWLwC4R2QHUxsdodw/Hjx/nwL5/EiVsMBGWKRPFipciffr00V9w6zvw7woyb+rGwJeqkTXrTbz6qkniXLduysoaG1eumDCBunWNRyhgUpYtbQ/ZykH9iZAuxK8yOhyOBPO8/WyV0AFiXZOzi4YFPd4wIlIWE2ke79xjMYw/Blinqp+KyACgC0aJrgFetLnPPgVWqeo3ts9oYJaqTott7Khrcn9u/518GY8Tlj7IclYlAFU4cSE95yU3pcuUj/nCc/thdjXIkIczDX6ldPmsVK0Kc+emnKxxMWMGtGnjlaXl8mmYWw/O74c7foVsZf0tosORagjUNbnEEOuanE3bMs/reEcSKrgMQGvgW9s0AiiDmco8wLUgwPiM2c0TqR8REXHducuXI8gY6hQcmOm8nGGXuRBXIFzmwsZh4/R2sm7pTt++yrx5xi0/UBg6FIoVs0VfI6/A8gfh1DZo8K1TcA5HKkdETlu/jajbaRE55csYvjiebBCRaomUNTpaYqy4QwCqekhVr6hqJCYJZ217nc/BhKo60gYL1gyNpjCai/+9hs/fRcFmJgXW3xPoeefnFCwIb7wRGEmcN2yAhQuhZ09bB2/ja7B/JtQYZuR2OBypGlXNpqrZo9myqapPwU0xKjkR8WiJasBq69m4zuYrSwprrhMwyet+hbzOtQU22/0ZQEcRySgipTDZrJNqTTBJOXnqDOHtnyO8/XMUuKkl4e2fo9sL78XaZ9+BI3z46Tcxnu/9+tCkFjP+VH4VCt9Fhk3P8/Eby1m8GBYs8LdQMGyYSbz8xBPArq9NXsqy3aHcM/4WzeFwJAMikl9Eins2n/rEtCYnIutUtbqIlInuvKr+lQhBswD/YDJZn7RtX2OmKhVTj+gpr7XA1zA1iyKAXqo6K657RF2T27L5N0rn8sm6TRKatu3BgunDrx57vudYHGxSnJ3Hs1Pp5lt9u/jSCZhdC718hhqvryV/icLMnp288sXGwYNQooQJHfh0wCr4+XbIexs0nQvpYnCmcTgcsRKoa3Ii0hqzhFUYkzKsBKaeXEy5la8S23SlgFFm0W2JEVhVz6pqHo+Cs20Pq+otqlpFVVt7p35R1YGqWkZVK/ii4AKJAYNG0f3F92n14Isc/vc4LTv0ovl9z/LgU/2IjIzkr117ebL3uwA0bPUUT/f5gDrhjzN/yWrAKEuAx3q+zfOvDqHJvc/w/rDxAKxau5l6d3aly7NvcVvLJ5L3QTLkhEbTkSun+bHPfSxacJFdu5L3ltFx4QJ88glUr248K3t33w9L20HmotBwmlNwDkdw8jZQF/hDVUthPPNX+dIxthCCfCLyQkwnVXVIvEQMEPr0L83G3xP3Q6VK5bP8982dPl9fvmwJPh/8MpGRkUwf/wFhYRl5/d3PWbpyA0UL57963bHjp3j71ac4e+4Cr7z9Gc0a1bpunPAmdRg6sDeN7unOy88/yvtDx/P9+A/IljUzlW7rmKhn8omcN0PdcRRbdj/Du/Rg1KhRvPtuylimEREm2Putt+DAAbj9dpg6+QJl9rWFy6egyVzImCdFZHE4HCnOZVU9KiLpRCSdqi4UEZ/WcmJTciFAVqxF50g41asYN/2z5y7Qo++HHDj4L4eOHKNyxdLXKbn8+XKRN3dOcmaP4OSpMzeMU7lCKUSETGEZAThz7jyFCuQFoFQJn0s0JY7i7aHya3RlIP1+qMzly72JKdQuKVCF//0PXnoJtm41JXMmToTGtyus6g57f4WG3xsF7HA4gpUTIpIVk85rgogcxuSyjJPYlNwBVX0rKaQLJOJjgSUV6cTMCs9esJJK5Uvy1fD+vDZwxA3x+97rddGtlUZdz8uaORMHDx8lW9bM7Nrtc/WKxFPlLQ5s30r/1n1YNb0C9R+4K+4+CWToUHjhBShfHn74AVq3tp6hWz6EXeON52extsl2f4fDERC0AS4AvTHJmXMAPumn2JScs+CSmNrVKzN4+ERWr99KliyZuLlitD49PvNyr0dp92hfypQqSrEiBeLukFRIOvK3+YotHzekar6OcGIl5Ixz/TfeREaa9bcGDYw351WLcdfXsKGvLX76epLf1+FwBAYiMhyYqKrLvZrHx2uMWLwrc6vqsUTI51f87V2ZEkRERBAaGsrpM+e495GXmP99/GrZxsu7Mho+GriXjjlrkbdABtLftdIEjychS5aYtbevvoKHH7aN++fA4laQvxE0/glCMibpPR2OtEygeVeKyPOY3MWFMKV8Jqnq+viMEVvR1FSr4NIKS1f9Rov7ehLevicvPJ3yWfYf6FKU1kP+R8TZo5yffZdxAElCxo2DrFltNhOAo2tg2X2Qo7IpfuoUnMMR1KjqMFWtB9wOHAXGiMg2EekvIrHkJLyGT0VTUyNpwZJLLIm15AAGD4alU2cz7dlWbDrSlNCmM7mlaoZEy3b2LBQsaPJRjh4NnNgEPzeG9NkhfAVkKhTXEA6HI54EmiUXHTYD1xigiqrGmX3dl7ReDkeMvPgijP7pTv737yiqFZzHlrGP8eMPia+C9P33cOYMdOkCnPoDFrSAkDBoNt8pOIcjjSEioSJyj4hMAGYB2wGfCnc7JedINHnyQJvej3G6zEA61JnI4ZlPM2SIJiq/5bhxULo0NKj6NyxoDhoJTedD1tJJJbbD4QhwRKSFrVazF3gS+B9QRlU7quqPvozhlJwjychW51Uul3+VJ5uOImRDL7p1U+IqdBAdu3cbb8oXnvwDmd/IlM9pOg9yVEx6oR0ORyDzCrACuMlmwpqoqj7Fx3lwSi4JaX7fs5w4efrqcZ/+H7N05YYbrgtv/xwRERF8PXUW6zZuv+7c11Nn8fXU6DOXnTh5mh9+Wnz1OCCSN0chfY130PK9eP7Oj6l4vg9160SybZvv/devh65doUrx3+hepiFcuQDNF0KuxK0dOhyO1IeqNlXVL1X1eELHcEouCWnZ7DZm/XytYPnKNZu5rfYtMV7/8AMtqV6lgs/jnzx1hh9nLbl6/NE7vRImaHIigtQYAuV78uJdQ+jfojO31bnA5Mmxd9u2DVq1MjkpMm2IlwAADwlJREFUQ48v45eBjQlJnwFaLIVcVVNGdofDEXQ4JZeE3NuyEf83dxkA6zdtp8pNZRgyYhIt7utJw1ZPsWHzH9dd/87gMSxYuoZLly5z/+Ov0LpzH2ba/pcvR1xN5tzxyde5cuUKoyf8HwuWriG8/XMcOXriavLmBUvX0Oie7jS6pzsLlq4BjLXY961PqX93N8ZNmpmC3wImJUmNYVD1A9rVmMzCfi3o+dRRpsVQy33fPmjWDFaujOTnTwYxq09jwrLnheZLIbvvPwIcDocjKrFlPAlKQnd8TLozOxI1RmTWskSUfe6G9jKlirL/4BEuXLjIjNlLad2yEbffVo3/PPsQf+3ay9uDxzDu03439JsxZyk1q95E3+ceocdLHxo5Q0P4ftwHZMqUkQGDRrFo+Tq6dr6HPfsOMfaTN67r/86QscycaAqpt36oD00b1gSgU7tw3urbjbsffIEunVol6pnjjQhUegmylKDKykfYPKgaPQcPJVvWttxx57VkOmfPmlRdGTnMtnGPke30T1C0PdT5EjLkSFmZHQ5H0JHmlFxy06xhLRYsW8uCpWt5+blH+PrbWUyePo90ki7GWnK7du/n1srlAKh2NZnzeXq89F/2HzzC4X+PU7ZUUcqWKhptfxEhezYT2hISci1spHKF0qRPH3o1d6ZfKNEByVKKvCufZGrP+5izuCULz/Tn5tsqkqdgDl54ajeP3jyEHi+OIuTsFag5HMo97cq4OxyOJCHNKbnoLLCkpM1djejT72OKFclPxowZGDn+B1bNGc3Ov/fxjLXSolKyeCE2bf2LO5vV47fNf1K7emXmLfqVcqWLMn54P/p/MApVCA0N5Upk5A39IyMjOXXaOBxduXItRi1g9ETe2oTcvZbT6z6lwcU3yHKpLiyCo2fy8Gn4SdKFQEjph+Cmvs6D0uFwJClpTsklN1UqlWXfgSM88XAbAGpWvYnm9/WkQZ2YvQNb39GQB5/qxz2dXyRnjmwA1KpWiUGffM26jdvJni0LZUsVpWD+3Bw/cYpO3d5g+Af/udr/td5duLuTKf3Xr0/XZHy6RJAulGw1e3GmZCfWLV/O2UN/oad3kiUkO9XbPQtZi/lbQofD4SMiEgKsAfapaisRKQVMBvIAa4GHVfWSiGQEvgJqYNJydVDVv1NUVpfWK+2SFGm9HA5H8OBrWi9bULsmkN0quanA96o6WUQ+B35T1REi8gwm/VZ3EekItFXVDsn7FNfjvCsdDofD4TMiUhS4G/jSHgvQFPD4T48H7rX7bbhWGmca0Exick5IJpySczgcDkd8GAq8BHgcBPIAJ1Q1wh7vBYrY/SLAHgB7/qS9PsXwm5ITkb9FZJOIbBCRNbYtt4jME5E/7Wcu2y4i8rGI7BCRjSJSPSH3DNKZ2QThvguHwxENoSKyxmvr5n1SRFoBh1V1rZ/kizf+djxpoqr/eh2/DMxX1fdF5GV73BdoCZSzWx1ghP30mfTpQ7kYIYSld3/dVeHEhfSEZcrkb1EcDkdgEaGqNWM5Xx9oLSJ3AWFAdmAYkFNEQq21VhTYZ6/fBxQD9opIKJAD44CSYvhbyUWlDdDY7o8HFmGUXBvgKzVeMqtEJKeIFFLVA74OnDd/YQ7sj3AWjCUsUyaKFS/lbzEcDkcqQlVfwSRNRkQaA31UtbOIfAu0x3hYPgp4KgTMsMcr7fkFmsLejv5UcgrMFREFvlDVkUABL8V1EChg96/O61o8c77XKTlrWncDyJDh+sKduXLlIleuXEn9DA6Hw+EwxshkEXkHWA+Mtu2jga9FZAdwDOiY0oL5U8k1UNV9IpIfmCci1+WqV1W1CtBnrKIcCSaEIOlEdTgcDoc3qroIM9uGqu4EakdzzQXg/hQVLAp+czxR1X328zAwHfMFHRKRQgD287C93DOv68F7ztfhcDgcjmjxiyUnIlmAdKp62u6HA29xbf72fW6c131WRCZjHE5OxrUed+7cORWRBJTsBMz3EhHnVcGFe+bgJ609L7hnji9B543mr+nKAsB0GxMYCkxU1dkishqYKiJdgd3AA/b6n4C7gB3AOeCxuG6gqgm2UkVkTRweRkGHe+bgJ609L7hndvhJydn52xvySanqUaBZNO0K9EgB0RwOh8MRRLiMJw6Hw+EIWpySi56R/hbAD7hnDn7S2vOCe+Y0T9BWIXA4HA6Hw1lyDofD4QhanJJzOBwOR9DilJwXInKniGy31Q5e9rc8yYGIFBORhSKyRUR+F5HnbXu0FSCCCREJEZH1IjLTHpcSkV/s+54iIhniGiM1YXO8ThORbSKyVUTqBft7FpHe9t/1ZhGZJCJhwfaeRWSMiBwWkc1ebclawSU145ScxZZzH46peFAJ6CQilfwrVbIQAbyoqpWAukAP+5yeChDlgPn2ONh4HtjqdfwB8JGqlgWOA139IlXyMQyYraoVMSE7Wwni9ywiRYDngJqqejMQgsmVGGzveRxwZ5S2mN6rdwWXbpgKLmkKp+SuURvYoao7VfUSJpt2Gz/LlOSo6gFVXWf3T2P+8BXh+gq+3pV9g4J4VjNO9YhIDqARNlGuql5S1RME+XvGxP5msmVdMmOSuAfVe1bVJZhkx97E9F6vVnBR1VWYkjiFUkbSwMApuWvEVOkgaBGRkkA14BdirgARLMSnmnEwUAo4Aoy1U7Rf2hR6QfuebT7c/wL/YJTbSWAtwf2ePcS3gkuawSm5NIqIZAW+A3qp6invczbDTNDElqTGasZJQChQHRihqtWAs0SZmgzC95wLY7mUAgoDWbhxWi/oCbb3mlickrtGmql0ICLpMQpugqp+b5tjqgARDHiqGf+NmYZuilc1Y3tNsL3vvcBeVf3FHk/DKL1gfs/NgV2qekRVLwPfY959ML9nD66CSww4JXeN1UA564mVAbNgPcPPMiU5di1qNLBVVYd4nfJUgIDrK0CkelT1FVUtqqolMe91gap2BhZiqhVD8D3zQWCPiFSwTc2ALQTxe8ZMU9YVkcz237nnmYP2PXsR03udATxivSzr4kMFl2DDZTzxQkTuwqzdhABjVHWgn0VKckSkAbAU2MS19alXMetyU4Hi2AoQqhp1cTvVIyKNgT6q2kpESmMsu9yYasYPqepFf8qXlIhIVYyjTQZgJ6Z6RzqC+D2LyJtAB4wX8XrgCcwaVNC8ZxGZBDQG8gKHgP7AD0TzXq2y/xQzbXsOeExV1/hDbn/hlJzD4XA4ghY3XelwOByOoMUpOYfD4XAELU7JORwOhyNocUrO4XA4HEGLU3IOh8PhCFqcknMkOyKiIjLY67iPiAxIorHHiUj7uK9M9H3ut5n8F0ZpL+nJBi8iVW0YSlLdM6eIPON1XFhEpsXWJ4H3GSoijURkuohssBnrT9r9DSJyW1Lf0963oIj8lBxjOxwenJJzpAQXgXYiktffgnjjlQXDF7oCT6pqk1iuqQrES8nFIUNO4KqSU9X9qpqkCl1E8gB1VXWJqrZV1aqY2LKlqlrVbiviIbPP2ID1oyJSJynGcziiwyk5R0oQAYwEekc9EdUSE5Ez9rOxiCwWkR9FZKeIvC8inUXkVxHZJCJlvIZpLiJrROQPm6fSUzvuQxFZbetoPeU17lIRmYHJhhFVnk52/M0i8oFt6wc0AEaLyIfRPaDNkvMW0MFaPx1EJIuY2l+/2iTJbey1XURkhogsAOaLSFYRmS8i6+y9PdUv3gfK2PE+jGI1honIWHv9ehFp4jX29yIyW0xtsUFe38c4+1ybRMTzLu4DZsf1AkVkr30H64G2IlJOROaIyFoRWSIi5e11Bez919jnrmvbm4rIb/ZZ1olJFg0miLlzXPd3OBKMqrrNbcm6AWeA7MDfQA6gDzDAnhsHtPe+1n42Bk4AhYCMmHx7b9pzzwNDvfrPxvxgK4fJ2RiGqZ31ur0mI7AGk7i3MSZZcalo5CyMSQ2VD5PgeAFwrz23CFOnLGqfksBmu98F+NTr3LuY7BpgrLI/MEmDu1g5c9tzoUB2u58X2AGI99jR3OtFTFYegIpW7jA79k77PYdhsl8UA2oA87zGymk/xwP3RHmmxsDMKG17gRe8jhcCZex+fWCu3Z+CsQyjyjsLqGP3swIhdr8EsN7f/0bdFrxbkkw7OBxxoaqnROQrTFHL8z52W602z56I/AXMte2bAO9pw6mqGgn8KSI7MX/0w4EqXlZiDowSvAT8qqq7orlfLWCRqh6x95yAqcn2g4/yRiUckxi6jz0Ow6RdAqNwPOm0BHhXRBphUq0VIe4SOA2ATwBUdZuI7AbK23PzVfWkfYYtGEXyO1BaRD4B/se177IQpiSPL0yxY+bEFNz9TkQ85zx/S5oDFbzac4lIJmA5MMx+p9+p6hl7/jDmx4XDkSw4JedISYYC64CxXm0R2GlzEUmHybPowTu/YKTXcSTX/9uNmptOMYqjp6rO8T4hJnfl2YSJH28EuE9Vt0eRoU4UGTpjrMcaqnpZTLWEsETc1/t7uwKEqupxEbkVuAPoDjwAPI75weHrvTwyC/CvmvW7qAhQW03hYW/esVPEdwOrRKSZqv5p7+3rjx6HI964NTlHimEtl6kYJw4Pf2Om0gBaA+kTMPT9IpLOrtOVBrYDc4CnxZQVQkTKe60DxcSvwO0ikldEQoBOwOJ4yHEayOZ1PAfoKdasEZFqMfTLgal3d9murZWIYTxvlmLXsux6WHHMc0eLdfpJp6rfAa9jyu6AqQxfNo7nug5VPQ4cEJG2dux0VoEC/Az08LpvVftZRlU3qup7mB86nuoI5YHN8bm/wxEfnJJzpDSDMetOHkZhFMtvQD0SZmX9g1FQs4DuqnoBk31/C7DOOmt8QRwzF3Zq9GXMetNvwFpVjU9ZloVAJY/jCfA2RmlvFJHf7XF0TABqisgm4BFgm5XnKLDcOotEdXj5DEhn+0wBumjsmfWLAItEZAPwDfCKbf8fZg0uvnQEutv39jvQyrb3AOpbZ58twJO2vY99jo2YNVrPdGkTK4PDkSy4KgQORxpHRJYBrVT1RArfVzAW6d2eNUSHI6lxSs7hSOPYNcLzqroxhe9bAONxGXTFiR2Bg1NyDofD4Qha3Jqcw+FwOIIWp+QcDofDEbQ4JedwOByOoMUpOYfD4XAELU7JORwOhyNo+X+7X3a0Qz1/3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data     = pd.read_csv('simulate_survival.csv')\n",
    "    X        = data[['x1','x2','x3']]\n",
    "    y_lower  = data['left']\n",
    "    y_higher = data['right']\n",
    "\n",
    "    param    = {'n_estimators' : 100,'learning_rate': 0.01,'Nestrov' : True,'subsample': 0.5,'min_samples_split': 10,\n",
    "                 'max_depth': 2,'metrics':'mae','dist':'normal','sigma':2,'random_state' : 0}\n",
    "\n",
    "    gb_manual = generate_result(X,y_lower,y_higher,param)\n",
    "    chart_creation(gb_manual,'Nesterov=True,Loss=Mae,Data=Mixed','Nesterov_True_Loss_Mae_Data_Mixed.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data     = pd.read_csv('../../data/ATAC_JV_adipose/inputs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = data.isna().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
