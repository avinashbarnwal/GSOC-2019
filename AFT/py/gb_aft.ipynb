{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                                   as np\n",
    "import pandas                                  as pd\n",
    "import matplotlib.pyplot                       as plt\n",
    "import math\n",
    "import random\n",
    "from   sklearn                                 import ensemble\n",
    "from   sklearn                                 import datasets\n",
    "from   sklearn.utils                           import shuffle\n",
    "from   sklearn.metrics                         import mean_squared_error\n",
    "from   sklearn.datasets                        import load_boston\n",
    "from   sklearn.model_selection                 import cross_val_score\n",
    "from   sklearn.tree                            import DecisionTreeRegressor\n",
    "from   sklearn.model_selection                 import train_test_split\n",
    "from   sklearn.ensemble._gradient_boosting     import predict_stages\n",
    "from   sklearn.ensemble._gradient_boosting     import predict_stage\n",
    "from   abc                                     import abstractmethod\n",
    "from   scipy.special                           import expit\n",
    "from   sklearn.utils                           import check_array\n",
    "from   sklearn.tree._tree                      import DTYPE\n",
    "from   sklearn.tree._tree                      import TREE_LEAF\n",
    "from   scipy.special                           import logsumexp\n",
    "from   sklearn.utils                           import check_random_state\n",
    "from   sklearn.ensemble.gradient_boosting      import ZeroEstimator\n",
    "from   _aft_loss                               import loss, negative_gradient,hessian\n",
    "import sys\n",
    "import graphviz\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from   sklearn.externals.six import StringIO  \n",
    "from   IPython.display import Image  \n",
    "from   sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "    \n",
    "    \"\"\"Abstract base class for various loss functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_classes : int\n",
    "        Number of classes\n",
    "    Attributes\n",
    "    ----------\n",
    "    K : int\n",
    "        The number of regression trees to be induced;\n",
    "        1 for regression and binary classification;\n",
    "        ``n_classes`` for multi-class classification.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    is_multi_class = False\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        self.K = n_classes\n",
    "\n",
    "    def init_estimator(self):\n",
    "        \n",
    "        \"\"\"Default ``init`` estimator for loss function. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, y_lower, y_higher,pred,dist,sigma,metrics,sample_weight=None):\n",
    "        \n",
    "        \"\"\"Compute the loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples,)\n",
    "            True labels\n",
    "        pred : array, shape (n_samples,)\n",
    "            Predicted labels\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            Sample weights.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def negative_gradient(self, y_lower, y_higher, pred,dist,sigma, **kargs):\n",
    "        \n",
    "        \"\"\"Compute the negative gradient.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples,)\n",
    "            The target labels.\n",
    "        pred : array, shape (n_samples,)\n",
    "            The predictions.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_terminal_regions(self, tree, X, y_lower,y_higher, residual, y_pred, dist, sigma, sample_weight, sample_mask, learning_rate=1.0):\n",
    "        \n",
    "        \"\"\"Update the terminal regions (=leaves) of the given tree and\n",
    "        updates the current predictions of the model. Traverses tree\n",
    "        and invokes template method '_update_terminal_region'.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : tree.Tree\n",
    "            The tree object.\n",
    "        X : array, shape (n, m)\n",
    "            The data array.\n",
    "        y : array, shape (n,)\n",
    "            The target labels.\n",
    "        residual : array, shape (n,)\n",
    "            The residuals (usually the negative gradient).\n",
    "        y_pred : array, shape (n,)\n",
    "            The predictions.\n",
    "        sample_weight : array, shape (n,)\n",
    "            The weight of each sample.\n",
    "        sample_mask : array, shape (n,)\n",
    "            The sample mask to be used.\n",
    "        learning_rate : float, default=0.1\n",
    "            learning rate shrinks the contribution of each tree by\n",
    "             ``learning_rate``.\n",
    "        k : int, default 0\n",
    "            The index of the estimator being updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute leaf for each sample in ''X''.\n",
    "        \n",
    "        terminal_regions                      = tree.apply(X)\n",
    "\n",
    "        # mask all which are not in sample mask.\n",
    "        masked_terminal_regions               = terminal_regions.copy()\n",
    "        masked_terminal_regions[~sample_mask] = -1\n",
    "\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            \n",
    "            self._update_terminal_region(tree, masked_terminal_regions,\n",
    "                                         leaf, X, y_lower, y_higher, residual, y_pred,dist,sigma, sample_weight)\n",
    "        \n",
    "        y_pred = y_pred + (learning_rate* tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
    "        return y_pred\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y_lower,y_higher,\n",
    "                                residual,pred,dist,sigma, sample_weight):\n",
    "        \n",
    "        \"\"\"Template method for updating terminal regions (=leaves).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroEstimator:\n",
    "    \n",
    "    \"\"\"An estimator that simply predicts zero.\n",
    "    .. deprecated:: 0.21\n",
    "        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version\n",
    "        0.21 and will be removed in version 0.23.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y_lower,y_higher,X_val, y_lower_val,y_higher_val, sample_weight=None):\n",
    "        \n",
    "        \"\"\"Fit the estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : numpy, shape (n_samples, n_targets)\n",
    "            Target values. Will be cast to X's dtype if necessary\n",
    "        sample_weight : array, shape (n_samples,)\n",
    "            Individual weights for each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.issubdtype(y_lower.dtype, np.signedinteger):\n",
    "            # classification\n",
    "            self.n_classes = np.unique(y_lower).shape[0]\n",
    "            if self.n_classes == 2:\n",
    "                self.n_classes = 1\n",
    "        else:\n",
    "            # regression\n",
    "            self.n_classes = 1\n",
    "\n",
    "    def predict(self, X,X_val):\n",
    "        \"\"\"Predict labels\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        #check_is_fitted(self, 'n_classes')\n",
    "\n",
    "        y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)\n",
    "        y.fill(0.0)\n",
    "        \n",
    "        y_val = np.empty((X_val.shape[0], self.n_classes), dtype=np.float64)\n",
    "        y_val.fill(0.0)\n",
    "        return y,y_val\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFT(LossFunction):\n",
    "    \"\"\"Cox Partial Likelihood\"\"\"\n",
    "\n",
    "    def __call__(self, y_lower, y_higher, y_pred, dist, sigma, metrics, sample_weight=None):\n",
    "        \"\"\"Compute the partial likelihood of prediction ``y_pred`` and ``y``.\"\"\"\n",
    "        # TODO add support for sample weights\n",
    "        return loss(y_lower, y_higher, y_pred.ravel(),dist, sigma,metrics)\n",
    "\n",
    "    def negative_gradient(self, y_lower, y_higher, y_pred,dist,sigma,k=0,sample_weight=None, **kwargs):\n",
    "        \"\"\"Negative gradient of partial likelihood\n",
    "        Parameters\n",
    "        ---------\n",
    "        y : tuple, len = 2\n",
    "            First element is boolean event indicator and second element survival/censoring time.\n",
    "        y_pred : np.ndarray, shape=(n,):\n",
    "            The predictions.\n",
    "        \"\"\"\n",
    "        ret = negative_gradient(y_lower, y_higher, y_pred.ravel(), dist, sigma)\n",
    "        if sample_weight is not None:\n",
    "            ret *= sample_weight\n",
    "        return ret\n",
    "\n",
    "    def init_estimator(self):  # pragma: no cover\n",
    "        return ZeroEstimator()\n",
    "\n",
    "\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y_lower,y_higher,\n",
    "                                residual, pred, dist, sigma, sample_weight):\n",
    "        \n",
    "        \"\"\"Least squares does not need to update terminal regions\"\"\"\n",
    "        \n",
    "        \"\"\"Make a single Newton-Raphson step.\n",
    "        our node estimate is given by:\n",
    "            sum(w * gradient) / sum(w * hessian)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        hess            = np.array(hessian(y_lower, y_higher, pred, dist, sigma))\n",
    "        terminal_region = np.where(terminal_regions == leaf)[0]\n",
    "        residual        = residual.take(terminal_region, axis=0)\n",
    "        hess            = hess.take(terminal_region, axis=0)\n",
    "        sample_weight   = sample_weight.take(terminal_region, axis=0)\n",
    "        pred            = pred.take(terminal_region, axis=0)\n",
    "\n",
    "        numerator       = np.sum(sample_weight * residual)\n",
    "        denominator     = np.sum(sample_weight * hess)\n",
    "\n",
    "        # prevents overflow and division by zero\n",
    "        \n",
    "        if abs(denominator) < 1e-2:\n",
    "            tree.value[leaf, 0, 0] = 0.0\n",
    "        else:\n",
    "            tree.value[leaf, 0, 0] = numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_sample_mask(n_total_samples,n_total_in_bag, random_state):\n",
    "    \n",
    "    \"\"\"Create a random sample mask where ``n_total_in_bag`` elements are set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_total_samples : int\n",
    "        The length of the resulting mask.\n",
    "\n",
    "    n_total_in_bag : int\n",
    "        The number of elements in the sample mask which are set to 1.\n",
    "        \n",
    "    random_state : np.RandomState\n",
    "        A numpy ``RandomState`` object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_mask : np.ndarray, shape=[n_total_samples]\n",
    "         An ndarray where ``n_total_in_bag`` elements are set to ``True``\n",
    "         the others are ``False``.\n",
    "    \"\"\"\n",
    "    \n",
    "    #random_state = np.random.RandomState(random_state)\n",
    "    rand         = random_state.rand(n_total_samples)\n",
    "    sample_mask  = np.zeros((n_total_samples,), dtype=np.bool)\n",
    "    n_bagged     = 0\n",
    "    \n",
    "    for i in range(n_total_samples):\n",
    "        \n",
    "        if rand[i] * (n_total_samples - i) < (n_total_in_bag - n_bagged):\n",
    "            sample_mask[i] = 1\n",
    "            n_bagged += 1\n",
    "            \n",
    "    return sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference between Attributes and Parameters\n",
    "\n",
    "class BaseGradientBoosting():\n",
    "    \"\"\"Abstract base class for Gradient Boosting. \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, loss, learning_rate, n_estimators, criterion,\n",
    "                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n",
    "                 max_depth, min_impurity_decrease, min_impurity_split,\n",
    "                 init, subsample, max_features,\n",
    "                 random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,\n",
    "                 warm_start=False, presort='auto', validation_fraction=0.25,\n",
    "                 n_iter_no_change=None,metrics = 'logloss', Nestrov=False,dist='normal',sigma =1,\n",
    "                 tol=1e-4):\n",
    "        \n",
    "        #Initial = 1\n",
    "        self.n_estimators             = n_estimators + 1\n",
    "        self.learning_rate            = learning_rate\n",
    "        self.loss                     = loss\n",
    "        self.criterion                = criterion\n",
    "        self.min_samples_split        = min_samples_split\n",
    "        self.min_samples_leaf         = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.subsample                = subsample\n",
    "        self.max_features             = max_features\n",
    "        self.max_depth                = max_depth\n",
    "        self.min_impurity_decrease    = min_impurity_decrease\n",
    "        self.min_impurity_split       = min_impurity_split\n",
    "        self.init                     = init\n",
    "        self.random_state             = random_state\n",
    "        self.alpha                    = alpha\n",
    "        self.verbose                  = verbose\n",
    "        self.max_leaf_nodes           = max_leaf_nodes\n",
    "        self.warm_start               = warm_start\n",
    "        self.presort                  = presort\n",
    "        self.validation_fraction      = validation_fraction\n",
    "        self.n_iter_no_change         = n_iter_no_change\n",
    "        self.tol                      = tol\n",
    "        self.Nestrov                  = Nestrov\n",
    "        self.dist                     = dist\n",
    "        self.sigma                    = sigma\n",
    "        self.metrics                  = metrics\n",
    "\n",
    "    #Very Important loss class is defined here.\n",
    "    \n",
    "    def _init_state(self):\n",
    "        \n",
    "        self.estimators_    = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.fitted_        = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.prev_valid_    = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.train_score_   = np.zeros((self.n_estimators, ),dtype=np.float64)\n",
    "        self.valid_score_   = np.zeros((self.n_estimators, ),dtype=np.float64)\n",
    "        self.random_state   = check_random_state(self.random_state)\n",
    "        \n",
    "        if self.Nestrov == True:\n",
    "            \n",
    "            self.g_fitted_      = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "            self.g_prev_valid_  = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "            self.lamb           = np.zeros((self.n_estimators,),dtype=np.float64)\n",
    "            self.gamma          = np.zeros((self.n_estimators,),dtype=np.float64)\n",
    "            self.gamma[0]       = 1\n",
    "            \n",
    "            for i in range(1,self.n_estimators):\n",
    "                self.lamb[i] = 0.5*(1+math.sqrt(1+4*self.lamb[i-1]**2))\n",
    "                \n",
    "            for i in range(1,self.n_estimators-1):\n",
    "                self.gamma[i] = (1-self.lamb[i])/self.lamb[i+1]\n",
    "                \n",
    "        \n",
    "        #do oob?\n",
    "        if self.init is None:\n",
    "            self.init_ = self.loss_.init_estimator()\n",
    "        elif isinstance(self.init, str):\n",
    "            self.init_ = INIT_ESTIMATORS[self.init]()\n",
    "        else:\n",
    "            self.init_ = self.init\n",
    "\n",
    "        \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n",
    "\n",
    "        if self.subsample < 1.0:\n",
    "            self.oob_improvement_ = np.zeros((self.n_estimators),dtype=np.float64)\n",
    "    \n",
    "    def _check_params(self):\n",
    "        \n",
    "        \"\"\"Check validity of parameters and raise ValueError if not valid. \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.loss == 'aft':\n",
    "            self.loss_ =  AFT(1)\n",
    "            \n",
    "\n",
    "    def _fit_stage(self, i, X, y_lower, y_higher, sample_weight, sample_mask, random_state):\n",
    "        \n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
    "        \n",
    "        assert sample_mask.dtype == np.bool\n",
    "        loss       = self.loss_\n",
    "        #original_y = y_lower\n",
    "        pred       = np.zeros((X.shape[0],self.loss_.K),dtype=np.float64)\n",
    "        \n",
    "        for k in range(loss.K):\n",
    "            if self.Nestrov == True:\n",
    "                pred[:,k] = self.g_fitted_[i-1,k]    \n",
    "            else:\n",
    "                pred[:,k] = self.fitted_[i-1,k]\n",
    "        \n",
    "        for k in range(loss.K):\n",
    "   \n",
    "            residual = loss.negative_gradient(y_lower,y_higher,pred,self.dist,self.sigma,k=k,sample_weight=sample_weight)\n",
    "        \n",
    "            # induce regression tree on residuals\n",
    "            tree     = DecisionTreeRegressor(\n",
    "                                            criterion                 = self.criterion,\n",
    "                                            splitter                  = 'best',\n",
    "                                            max_depth                 = self.max_depth,\n",
    "                                            min_samples_split         = self.min_samples_split,\n",
    "                                            min_samples_leaf          = self.min_samples_leaf,\n",
    "                                            min_weight_fraction_leaf  = self.min_weight_fraction_leaf,\n",
    "                                            min_impurity_decrease     = self.min_impurity_decrease,\n",
    "                                            min_impurity_split        = self.min_impurity_split,\n",
    "                                            max_features              = self.max_features,\n",
    "                                            max_leaf_nodes            = self.max_leaf_nodes,\n",
    "                                            random_state              = random_state,\n",
    "                                            presort                   = self.presort\n",
    "                                            )\n",
    "\n",
    "            if self.subsample < 1.0:\n",
    "                # no inplace multiplication!\n",
    "                sample_weight = sample_weight * sample_mask.astype(np.float64)\n",
    "                \n",
    "\n",
    "            tree.fit(X, residual, sample_weight=sample_weight)\n",
    "            \n",
    "            #dot_data = StringIO()\n",
    "            #export_graphviz(tree, out_file=dot_data,  \n",
    "            #                filled=True, rounded=True,\n",
    "            #                special_characters=True)\n",
    "            #graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "            #Image(graph.create_png())\n",
    "            #filename = \"tree\"+str(i)+\".png\"\n",
    "            #graph.write_png(filename)\n",
    "\n",
    "            # update tree leaves    \n",
    "            if self.Nestrov == True:\n",
    "                \n",
    "                y_pred                  = self.g_fitted_[i-1,k]\n",
    "                self.fitted_[i,k]       = loss.update_terminal_regions(tree.tree_, X, y_lower, y_higher, residual, y_pred,self.dist,self.sigma,sample_weight, sample_mask,self.learning_rate)\n",
    "                self.g_fitted_[i,k]     = (1-self.gamma[i-1])*self.fitted_[i,k]+self.gamma[i-1]*self.fitted_[i-1,k]\n",
    "                \n",
    "            else:\n",
    "                y_pred            = self.fitted_[i-1,k]\n",
    "                self.fitted_[i,k] = loss.update_terminal_regions(tree.tree_, X, y_lower,y_higher, residual, y_pred,self.dist,self.sigma,sample_weight, sample_mask,self.learning_rate)\n",
    "\n",
    "            # add tree to ensemble\n",
    "            self.estimators_[i, k] = tree\n",
    "    \n",
    "    def n_features(self):\n",
    "        return self.n_features_\n",
    "    \n",
    "    def _validate_y(self, y, sample_weight):\n",
    "        self.classes_    = np.unique(y)\n",
    "        self.n_classes_  = len(self.classes_)\n",
    "        return y\n",
    "    \n",
    "    def _fit_stages(self, X, y_lower, y_higher,sample_weight, random_state,\n",
    "                    X_val, y_lower_val, y_higher_val,sample_weight_val,begin_at_stage=0):\n",
    "        \n",
    "        \n",
    "        n_samples    = X.shape[0]\n",
    "        do_oob       = self.subsample < 1.0\n",
    "        sample_mask  = np.ones((n_samples, ), dtype=np.bool)\n",
    "        n_inbag      = max(1, int(self.subsample * n_samples))\n",
    "        loss_        = self.loss_\n",
    "        \n",
    "        # create one-hot label encoding\n",
    "        pred         = np.zeros((n_samples, self.loss_.K), dtype=np.float64)\n",
    "        pred_val     = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "        \n",
    "        for k in range(self.loss_.K):\n",
    "            pred[:,k] = self.fitted_[0,k]\n",
    "            pred_val[:,k] = self.prev_valid_[0,k]\n",
    "            \n",
    "        if do_oob:\n",
    "            \n",
    "            sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n",
    "            self.train_score_[0] = loss_(y_lower[sample_mask],y_higher[sample_mask],pred[sample_mask],self.dist,self.sigma,self.metrics,sample_weight[sample_mask])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.train_score_[0] = loss_(y_lower,y_higher,pred,self.dist,self.sigma,self.metrics,sample_weight)\n",
    "        self.valid_score_[0] = loss_(y_lower_val,y_higher_val,pred_val,self.dist,self.sigma,self.metrics,sample_weight_val)\n",
    "\n",
    "        # perform boosting iterations\n",
    "        # validation loss performance\n",
    "        \n",
    "        for i in range(begin_at_stage, self.n_estimators):\n",
    "\n",
    "            # subsampling\n",
    "            if do_oob:\n",
    "                sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n",
    "                \n",
    "            # fit next stage of trees\n",
    "            self._fit_stage(i, X, y_lower,y_higher, sample_weight,sample_mask, random_state)\n",
    "\n",
    "            if self.Nestrov == True:\n",
    "                \n",
    "                score = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "                \n",
    "                for k in range(self.loss_.K):\n",
    "                    score[:,k] = self.g_prev_valid_[i-1,k].copy()\n",
    "                    \n",
    "                predict_stage(self.estimators_, i, X_val, self.learning_rate, score)\n",
    "\n",
    "                for k in range(self.loss_.K):\n",
    "                    self.prev_valid_[i,k] = score[:,k].copy()\n",
    "                    \n",
    "                for k in range(self.loss_.K):\n",
    "                    self.g_prev_valid_[i,k] = (1-self.gamma[i-1])*self.prev_valid_[i,k]+self.gamma[i-1]*self.prev_valid_[i-1,k]\n",
    "            else:\n",
    "                \n",
    "                score = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "                for k in range(self.loss_.K):\n",
    "                    score[:,k] = self.prev_valid_[i-1,k].copy()\n",
    "\n",
    "                predict_stage(self.estimators_, i, X_val, self.learning_rate, score)\n",
    "                \n",
    "                for k in range(self.loss_.K):\n",
    "                    self.prev_valid_[i,k] = score[:,k].copy()\n",
    "\n",
    "            for k in range(self.loss_.K):\n",
    "                \n",
    "                pred[:,k] = self.fitted_[i,k]\n",
    "                pred_val[:,k] = self.prev_valid_[i,k]\n",
    "            \n",
    "            if do_oob:\n",
    "                self.train_score_[i] = loss_(y_lower[sample_mask],y_higher[sample_mask],pred[sample_mask],self.dist,self.sigma,self.metrics,sample_weight[sample_mask])\n",
    "            else:\n",
    "                self.train_score_[i] = loss_(y_lower,y_higher,pred,self.dist,self.sigma,self.metrics,sample_weight)\n",
    "   \n",
    "            self.valid_score_[i] = loss_(y_lower_val,y_higher_val,pred_val,self.dist,self.sigma,self.metrics,sample_weight_val)\n",
    "    \n",
    "        return i + 1\n",
    "\n",
    "    \n",
    "    def fit(self, X, y_lower,y_higher, sample_weight=None):\n",
    "        \n",
    "        # Check input\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        #y_lower                     = self._validate_y(y_lower, sample_weight)\n",
    "        #y_higher                    = self._validate_y(y_higher, sample_weight)\n",
    "        X                           = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        sample_weight               = np.ones(n_samples, dtype=np.float32)\n",
    "        \n",
    "        X, X_val, y_lower, y_lower_val,y_higher,y_higher_val,sample_weight, sample_weight_val \\\n",
    "        = train_test_split(X, y_lower,y_higher,sample_weight,random_state=self.random_state,test_size=self.validation_fraction)\n",
    "        self._check_params()\n",
    "        self._init_state()\n",
    "\n",
    "        # fit initial model - FIXME make sample_weight optional\n",
    "        #For Binomial       - init_ = LogOddsEstimator\n",
    "        #For Multinomial    - init_ = PriorProbabilityEstimator\n",
    "\n",
    "        self.init_.fit(X, y_lower,y_higher, X_val, y_lower_val,y_higher_val, sample_weight)\n",
    "        # init predictions and update in the inplace self\n",
    "        initial_pred,initial_val_pred  = self.init_.predict(X,X_val)\n",
    "        \n",
    "        for k in range(self.loss_.K):\n",
    "            self.fitted_[0,k], self.prev_valid_[0,k]          = initial_pred[:,k],initial_val_pred[:,k]\n",
    "            if self.Nestrov == True:\n",
    "                self.g_fitted_[0,k], self.g_prev_valid_[0,k]  = initial_pred[:,k],initial_val_pred[:,k]\n",
    "\n",
    "        begin_at_stage = 1\n",
    "        # fit the boosting stages\n",
    "        \n",
    "        n_stages = self._fit_stages(X, y_lower,y_higher, sample_weight, self.random_state,\n",
    "                                    X_val, y_lower_val,y_higher_val, sample_weight_val,begin_at_stage)\n",
    "        # change shape of arrays after fit (early-stopping or additional ests)\n",
    "        self.n_estimators_ = n_stages\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _make_estimator(self, append=True):\n",
    "        # we don't need _make_estimator\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def _init_decision_function(self, X):\n",
    "        \n",
    "        \"\"\"Check input and compute prediction of ``init``. \"\"\"\n",
    "        #self._check_initialized()\n",
    "        #X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
    "                self.n_features_, X.shape[1]))\n",
    "        score = self.init_.predict(X).astype(np.float64)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _decision_function(self, X):\n",
    "        \n",
    "        # for use in inner loop, not raveling the output in single-class case,\n",
    "        # not doing input validation.\n",
    "        \n",
    "        score = self._init_decision_function(X)\n",
    "        predict_stages(self.estimators_, X, self.learning_rate, score)\n",
    "        return score\n",
    "\n",
    "    def _staged_decision_function(self, X):\n",
    "        \n",
    "        #X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        \n",
    "        score = self._init_decision_function(X)\n",
    "        for i in range(self.estimators_.shape[0]):\n",
    "            predict_stage(self.estimators_, i, X, self.learning_rate, score)\n",
    "            yield score.copy()\n",
    "    \n",
    "\n",
    "\n",
    "class GradientBoostingClassifier(BaseGradientBoosting):\n",
    "\n",
    "    _SUPPORTED_LOSS = ('survival')\n",
    "\n",
    "    def __init__(self, loss='aft', learning_rate=0.1, n_estimators=100,\n",
    "                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n",
    "                 max_depth=3, min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None, init=None,\n",
    "                 random_state=None, max_features=None, verbose=0,\n",
    "                 max_leaf_nodes=None, warm_start=False,\n",
    "                 presort='auto', validation_fraction=0.25,\n",
    "                 n_iter_no_change=None,Nestrov=False,metrics = 'logloss',dist='normal',sigma=1,tol=1e-4):\n",
    "\n",
    "        super().__init__(\n",
    "            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n",
    "            criterion=criterion, min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_depth=max_depth, init=init, subsample=subsample,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state, verbose=verbose,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            warm_start=warm_start, presort=presort,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change,Nestrov=Nestrov,metrics=metrics,\n",
    "            dist=dist,sigma=sigma,tol=tol)\n",
    "\n",
    "    def _validate_y(self, y, sample_weight):\n",
    "        #check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n",
    "        if n_trim_classes < 2:\n",
    "            raise ValueError(\"y contains %d class after sample_weight \"\n",
    "                             \"trimmed classes with zero weights, while a \"\n",
    "                             \"minimum of 2 classes are required.\"\n",
    "                             % n_trim_classes)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        return y\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        score = self._decision_function(X)\n",
    "        if score.shape[1] == 1:\n",
    "            return score.ravel()\n",
    "        return score\n",
    "\n",
    "    #def staged_decision_function(self, X):\n",
    "    #    \n",
    "    #    yield from self._staged_decision_function(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "       \n",
    "        score     = self.decision_function(X)\n",
    "        decisions = self.loss_._score_to_decision(score)\n",
    "        return self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def staged_predict(self, X):\n",
    "       \n",
    "        for score in self._staged_decision_function(X):\n",
    "            decisions = self.loss_._score_to_decision(score)\n",
    "            yield self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        score = self.decision_function(X)\n",
    "        try:\n",
    "            return self.loss_._score_to_proba(score)\n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \n",
    "        proba = self.predict_proba(X)\n",
    "        return np.log(proba)\n",
    "\n",
    "    def staged_predict_proba(self, X):\n",
    "       \n",
    "        try:\n",
    "            for score in self._staged_decision_function(X):\n",
    "                yield self.loss_._score_to_proba(score)\n",
    "                \n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "def data_creation(file_name,K=5):\n",
    "    \n",
    "    df_train   = pd.read_csv(file_name)\n",
    "    y          = list(df_train['Y'])\n",
    "    req_cols   = [i for i in df_train.columns if i != 'Y']\n",
    "    X          = np.array(df_train[req_cols])\n",
    "    y_median   = np.percentile(y, 50) # return 50th percentile, e.g median.\n",
    "    bin_y      = list(map(lambda x : 0 if x < y_median else 1,y))\n",
    "\n",
    "    percentile = np.percentile(y, np.arange(0, 100, 100/K)) # deciles\n",
    "    multi_y    = list(map(lambda x : 0 if x >= percentile[0] and x< percentile[1] else 1 if x >= percentile[1] and x< percentile[2] else 2 if x >= percentile[2] and x< percentile[3] else 3,y))\n",
    "    \n",
    "    return X,y,bin_y,multi_y\n",
    "\n",
    "def chart_creation(gb,chart_title,chart_name):\n",
    "    \n",
    "    min_valid = round(np.min(gb.valid_score_),4)\n",
    "    min_train = round(np.min(gb.train_score_),4)\n",
    "    min_iter  = round(np.nanargmin(gb.valid_score_),0)\n",
    "\n",
    "    textstr = '\\n'.join((\n",
    "                    'Min Train = %.2f' % (min_train, ),\n",
    "                    'Min Valid = %.2f' % (min_valid, ),\n",
    "                    'Min Iter  = %.2f' % (min_iter, )))\n",
    "\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5,edgecolor=\"black\")\n",
    "    \n",
    "    fig,ax1       = plt.subplots()\n",
    "    ax2           = ax1.twinx()\n",
    "\n",
    "    ln1 = ax1.plot(gb.train_score_,color='blue',label='Training')\n",
    "    ln2 = ax2.plot(gb.valid_score_,color='orange',label='Validation')\n",
    "    \n",
    "    #ax1.axvline(x=np.nanargmin(gb.valid_score_),color='r')\n",
    "    #ax2.axhline(y=np.min(gb.valid_score_),color='b')\n",
    "    lns = ln1 + ln2\n",
    "    \n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='lower left',fancybox='round', facecolor='wheat',fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel(\"Number of Iterations(Trees)\")\n",
    "    ax1.set_ylabel(\"Training Negative Likelihood(Loss)\")\n",
    "    ax2.set_ylabel(\"Validation Negative Likelihood(Loss)\")\n",
    "    #ax1.legend([\"Training\",\"Validation\"],loc='lower left',fancybox='round', facecolor='wheat',fontsize=8)\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax2.text(0.7, 0.90, textstr, transform=ax1.transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "    plt.title(chart_title)\n",
    "    plt.show()\n",
    "    fig.savefig(chart_name)\n",
    "    \n",
    "def generate_result(X,y_lower,y_higher,param):\n",
    "    \n",
    "    gb_manual = GradientBoostingClassifier(**param)\n",
    "    gb_manual.fit(X,y_lower,y_higher)\n",
    "    \n",
    "    return gb_manual    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data     = pd.read_csv('simulate_survival.csv')\n",
    "    X        = data[['x1','x2','x3']]\n",
    "    y_lower  = data['left']\n",
    "    y_higher = data['right']\n",
    "\n",
    "    param    = {'n_estimators' : 100,'learning_rate': 0.01,'Nestrov' : True,'subsample': 0.5,'min_samples_split': 10,\n",
    "                 'max_depth': 2,'metrics':'logloss','dist':'normal','sigma':2,'random_state' : 0}\n",
    "\n",
    "    gb_manual = generate_result(X,y_lower,y_higher,param)\n",
    "    chart_creation(gb_manual,'Nesterov=True,Loss=LogLoss,Data=Mixed','Nesterov_True_Loss_LogLoss_Data_Mixed.png')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
