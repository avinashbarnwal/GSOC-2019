{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                                   as np\n",
    "import pandas                                  as pd\n",
    "import matplotlib.pyplot                       as plt\n",
    "import math\n",
    "import random\n",
    "from   sklearn                                 import ensemble\n",
    "from   sklearn                                 import datasets\n",
    "from   sklearn.utils                           import shuffle\n",
    "from   sklearn.metrics                         import mean_squared_error\n",
    "from   sklearn.datasets                        import load_boston\n",
    "from   sklearn.model_selection                 import cross_val_score\n",
    "from   sklearn.tree                            import DecisionTreeRegressor\n",
    "from   sklearn.model_selection                 import train_test_split\n",
    "from   sklearn.ensemble._gradient_boosting     import predict_stages\n",
    "from   sklearn.ensemble._gradient_boosting     import predict_stage\n",
    "from   abc                                     import abstractmethod\n",
    "from   scipy.special                           import expit\n",
    "from   sklearn.utils                           import check_array\n",
    "from   sklearn.tree._tree                      import DTYPE\n",
    "from   sklearn.tree._tree                      import TREE_LEAF\n",
    "from   scipy.special                           import logsumexp\n",
    "from   sklearn.utils                           import check_random_state\n",
    "from   sklearn.ensemble.gradient_boosting      import ZeroEstimator\n",
    "from   _aft_loss                               import loss, negative_gradient,hessian\n",
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction(object):\n",
    "    \n",
    "    \"\"\"Abstract base class for various loss functions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_classes : int\n",
    "        Number of classes\n",
    "    Attributes\n",
    "    ----------\n",
    "    K : int\n",
    "        The number of regression trees to be induced;\n",
    "        1 for regression and binary classification;\n",
    "        ``n_classes`` for multi-class classification.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    is_multi_class = False\n",
    "\n",
    "    def __init__(self, n_classes):\n",
    "        self.K = n_classes\n",
    "\n",
    "    def init_estimator(self):\n",
    "        \n",
    "        \"\"\"Default ``init`` estimator for loss function. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def __call__(self, y_lower, y_higher,pred,dist,sigma,metrics,sample_weight=None):\n",
    "        \n",
    "        \"\"\"Compute the loss.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples,)\n",
    "            True labels\n",
    "        pred : array, shape (n_samples,)\n",
    "            Predicted labels\n",
    "        sample_weight : array-like, shape (n_samples,), optional\n",
    "            Sample weights.\n",
    "        \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def negative_gradient(self, y_lower, y_higher, pred,dist,sigma, **kargs):\n",
    "        \n",
    "        \"\"\"Compute the negative gradient.\n",
    "        Parameters\n",
    "        ----------\n",
    "        y : array, shape (n_samples,)\n",
    "            The target labels.\n",
    "        pred : array, shape (n_samples,)\n",
    "            The predictions.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    def update_terminal_regions(self, tree, X, y_lower,y_higher, residual, y_pred, dist, sigma, sample_weight, sample_mask, learning_rate=1.0):\n",
    "        \n",
    "        \"\"\"Update the terminal regions (=leaves) of the given tree and\n",
    "        updates the current predictions of the model. Traverses tree\n",
    "        and invokes template method '_update_terminal_region'.\n",
    "        Parameters\n",
    "        ----------\n",
    "        tree : tree.Tree\n",
    "            The tree object.\n",
    "        X : array, shape (n, m)\n",
    "            The data array.\n",
    "        y : array, shape (n,)\n",
    "            The target labels.\n",
    "        residual : array, shape (n,)\n",
    "            The residuals (usually the negative gradient).\n",
    "        y_pred : array, shape (n,)\n",
    "            The predictions.\n",
    "        sample_weight : array, shape (n,)\n",
    "            The weight of each sample.\n",
    "        sample_mask : array, shape (n,)\n",
    "            The sample mask to be used.\n",
    "        learning_rate : float, default=0.1\n",
    "            learning rate shrinks the contribution of each tree by\n",
    "             ``learning_rate``.\n",
    "        k : int, default 0\n",
    "            The index of the estimator being updated.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute leaf for each sample in ''X''.\n",
    "        \n",
    "        terminal_regions                      = tree.apply(X)\n",
    "\n",
    "        # mask all which are not in sample mask.\n",
    "        masked_terminal_regions               = terminal_regions.copy()\n",
    "        masked_terminal_regions[~sample_mask] = -1\n",
    "\n",
    "        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:\n",
    "            \n",
    "            self._update_terminal_region(tree, masked_terminal_regions,\n",
    "                                         leaf, X, y_lower, y_higher, residual, y_pred,dist,sigma, sample_weight)\n",
    "        \n",
    "        y_pred = y_pred + (learning_rate* tree.value[:, 0, 0].take(terminal_regions, axis=0))\n",
    "        return y_pred\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y_lower,y_higher,\n",
    "                                residual,pred,dist,sigma, sample_weight):\n",
    "        \n",
    "        \"\"\"Template method for updating terminal regions (=leaves).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroEstimator:\n",
    "    \n",
    "    \"\"\"An estimator that simply predicts zero.\n",
    "    .. deprecated:: 0.21\n",
    "        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version\n",
    "        0.21 and will be removed in version 0.23.\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y_lower,y_higher,X_val, y_lower_val,y_higher_val, sample_weight=None):\n",
    "        \n",
    "        \"\"\"Fit the estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : numpy, shape (n_samples, n_targets)\n",
    "            Target values. Will be cast to X's dtype if necessary\n",
    "        sample_weight : array, shape (n_samples,)\n",
    "            Individual weights for each sample\n",
    "        \"\"\"\n",
    "        \n",
    "        if np.issubdtype(y_lower.dtype, np.signedinteger):\n",
    "            # classification\n",
    "            self.n_classes = np.unique(y_lower).shape[0]\n",
    "            if self.n_classes == 2:\n",
    "                self.n_classes = 1\n",
    "        else:\n",
    "            # regression\n",
    "            self.n_classes = 1\n",
    "\n",
    "    def predict(self, X,X_val):\n",
    "        \"\"\"Predict labels\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
    "            Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        y : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        #check_is_fitted(self, 'n_classes')\n",
    "\n",
    "        y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)\n",
    "        y.fill(0.0)\n",
    "        \n",
    "        y_val = np.empty((X_val.shape[0], self.n_classes), dtype=np.float64)\n",
    "        y_val.fill(0.0)\n",
    "        return y,y_val\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFT(LossFunction):\n",
    "    \"\"\"Cox Partial Likelihood\"\"\"\n",
    "\n",
    "    def __call__(self, y_lower, y_higher, y_pred, dist, sigma, metrics, sample_weight=None):\n",
    "        \"\"\"Compute the partial likelihood of prediction ``y_pred`` and ``y``.\"\"\"\n",
    "        # TODO add support for sample weights\n",
    "        return loss(y_lower, y_higher, y_pred.ravel(),dist, sigma,metrics)\n",
    "\n",
    "    def negative_gradient(self, y_lower, y_higher, y_pred,dist,sigma,k=0,sample_weight=None, **kwargs):\n",
    "        \"\"\"Negative gradient of partial likelihood\n",
    "        Parameters\n",
    "        ---------\n",
    "        y : tuple, len = 2\n",
    "            First element is boolean event indicator and second element survival/censoring time.\n",
    "        y_pred : np.ndarray, shape=(n,):\n",
    "            The predictions.\n",
    "        \"\"\"\n",
    "        ret = negative_gradient(y_lower, y_higher, y_pred.ravel(), dist, sigma)\n",
    "        if sample_weight is not None:\n",
    "            ret *= sample_weight\n",
    "        return ret\n",
    "\n",
    "    def init_estimator(self):  # pragma: no cover\n",
    "        return ZeroEstimator()\n",
    "\n",
    "\n",
    "    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y_lower,y_higher,\n",
    "                                residual, pred, dist, sigma, sample_weight):\n",
    "        \n",
    "        \"\"\"Least squares does not need to update terminal regions\"\"\"\n",
    "        \n",
    "        \"\"\"Make a single Newton-Raphson step.\n",
    "        our node estimate is given by:\n",
    "            sum(w * gradient) / sum(w * hessian)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        hess            = np.array(hessian(y_lower, y_higher, pred, dist, sigma))\n",
    "        terminal_region = np.where(terminal_regions == leaf)[0]\n",
    "        residual        = residual.take(terminal_region, axis=0)\n",
    "        hess            = hess.take(terminal_region, axis=0)\n",
    "        sample_weight   = sample_weight.take(terminal_region, axis=0)\n",
    "        pred            = pred.take(terminal_region, axis=0)\n",
    "\n",
    "        numerator       = np.sum(sample_weight * residual)\n",
    "        denominator     = np.sum(sample_weight * hess)\n",
    "\n",
    "        # prevents overflow and division by zero\n",
    "        \n",
    "        if abs(denominator) < 1e-2:\n",
    "            tree.value[leaf, 0, 0] = 0.0\n",
    "        else:\n",
    "            tree.value[leaf, 0, 0] = numerator/denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_sample_mask(n_total_samples,n_total_in_bag, random_state):\n",
    "    \n",
    "    \"\"\"Create a random sample mask where ``n_total_in_bag`` elements are set.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_total_samples : int\n",
    "        The length of the resulting mask.\n",
    "\n",
    "    n_total_in_bag : int\n",
    "        The number of elements in the sample mask which are set to 1.\n",
    "        \n",
    "    random_state : np.RandomState\n",
    "        A numpy ``RandomState`` object.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sample_mask : np.ndarray, shape=[n_total_samples]\n",
    "         An ndarray where ``n_total_in_bag`` elements are set to ``True``\n",
    "         the others are ``False``.\n",
    "    \"\"\"\n",
    "    \n",
    "    #random_state = np.random.RandomState(random_state)\n",
    "    rand         = random_state.rand(n_total_samples)\n",
    "    sample_mask  = np.zeros((n_total_samples,), dtype=np.bool)\n",
    "    n_bagged     = 0\n",
    "    \n",
    "    for i in range(n_total_samples):\n",
    "        \n",
    "        if rand[i] * (n_total_samples - i) < (n_total_in_bag - n_bagged):\n",
    "            sample_mask[i] = 1\n",
    "            n_bagged += 1\n",
    "            \n",
    "    return sample_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference between Attributes and Parameters\n",
    "\n",
    "class BaseGradientBoosting():\n",
    "    \"\"\"Abstract base class for Gradient Boosting. \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, loss, learning_rate, n_estimators, criterion,\n",
    "                 min_samples_split, min_samples_leaf, min_weight_fraction_leaf,\n",
    "                 max_depth, min_impurity_decrease, min_impurity_split,\n",
    "                 init, subsample, max_features,\n",
    "                 random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,\n",
    "                 warm_start=False, presort='auto', validation_fraction=0.25,\n",
    "                 n_iter_no_change=None,metrics = 'logloss', Nestrov=False,dist='normal',sigma =1,\n",
    "                 tol=1e-4):\n",
    "        \n",
    "        #Initial = 1\n",
    "        self.n_estimators             = n_estimators + 1\n",
    "        self.learning_rate            = learning_rate\n",
    "        self.loss                     = loss\n",
    "        self.criterion                = criterion\n",
    "        self.min_samples_split        = min_samples_split\n",
    "        self.min_samples_leaf         = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.subsample                = subsample\n",
    "        self.max_features             = max_features\n",
    "        self.max_depth                = max_depth\n",
    "        self.min_impurity_decrease    = min_impurity_decrease\n",
    "        self.min_impurity_split       = min_impurity_split\n",
    "        self.init                     = init\n",
    "        self.random_state             = random_state\n",
    "        self.alpha                    = alpha\n",
    "        self.verbose                  = verbose\n",
    "        self.max_leaf_nodes           = max_leaf_nodes\n",
    "        self.warm_start               = warm_start\n",
    "        self.presort                  = presort\n",
    "        self.validation_fraction      = validation_fraction\n",
    "        self.n_iter_no_change         = n_iter_no_change\n",
    "        self.tol                      = tol\n",
    "        self.Nestrov                  = Nestrov\n",
    "        self.dist                     = dist\n",
    "        self.sigma                    = sigma\n",
    "        self.metrics                  = metrics\n",
    "\n",
    "    #Very Important loss class is defined here.\n",
    "    \n",
    "    def _init_state(self):\n",
    "        \n",
    "        self.estimators_    = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.fitted_        = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.prev_valid_    = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "        self.train_score_   = np.zeros((self.n_estimators, ),dtype=np.float64)\n",
    "        self.valid_score_   = np.zeros((self.n_estimators, ),dtype=np.float64)\n",
    "        self.random_state   = check_random_state(self.random_state)\n",
    "        \n",
    "        if self.Nestrov == True:\n",
    "            \n",
    "            self.g_fitted_      = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "            self.g_prev_valid_  = np.empty((self.n_estimators, self.loss_.K),dtype=np.object)\n",
    "            self.lamb           = np.zeros((self.n_estimators,),dtype=np.float64)\n",
    "            self.gamma          = np.zeros((self.n_estimators,),dtype=np.float64)\n",
    "            self.gamma[0]       = 1\n",
    "            \n",
    "            for i in range(1,self.n_estimators):\n",
    "                self.lamb[i] = 0.5*(1+math.sqrt(1+4*self.lamb[i-1]**2))\n",
    "                \n",
    "            for i in range(1,self.n_estimators-1):\n",
    "                self.gamma[i] = (1-self.lamb[i])/self.lamb[i+1]\n",
    "                \n",
    "        \n",
    "        #do oob?\n",
    "        if self.init is None:\n",
    "            self.init_ = self.loss_.init_estimator()\n",
    "        elif isinstance(self.init, str):\n",
    "            self.init_ = INIT_ESTIMATORS[self.init]()\n",
    "        else:\n",
    "            self.init_ = self.init\n",
    "\n",
    "        \"\"\"Initialize model state and allocate model state data structures. \"\"\"\n",
    "\n",
    "        if self.subsample < 1.0:\n",
    "            self.oob_improvement_ = np.zeros((self.n_estimators),dtype=np.float64)\n",
    "    \n",
    "    def _check_params(self):\n",
    "        \n",
    "        \"\"\"Check validity of parameters and raise ValueError if not valid. \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.loss == 'aft':\n",
    "            self.loss_ =  AFT(1)\n",
    "            \n",
    "\n",
    "    def _fit_stage(self, i, X, y_lower, y_higher, sample_weight, sample_mask, random_state):\n",
    "        \n",
    "        \"\"\"Fit another stage of ``n_classes_`` trees to the boosting model. \"\"\"\n",
    "        \n",
    "        assert sample_mask.dtype == np.bool\n",
    "        loss       = self.loss_\n",
    "        #original_y = y_lower\n",
    "        pred       = np.zeros((X.shape[0],self.loss_.K),dtype=np.float64)\n",
    "        \n",
    "        for k in range(loss.K):\n",
    "            if self.Nestrov == True:\n",
    "                pred[:,k] = self.g_fitted_[i-1,k]    \n",
    "            else:\n",
    "                pred[:,k] = self.fitted_[i-1,k]\n",
    "        \n",
    "        for k in range(loss.K):\n",
    "   \n",
    "            residual = loss.negative_gradient(y_lower,y_higher,pred,self.dist,self.sigma,k=k,sample_weight=sample_weight)\n",
    "        \n",
    "            # induce regression tree on residuals\n",
    "            tree     = DecisionTreeRegressor(\n",
    "                                            criterion                 = self.criterion,\n",
    "                                            splitter                  = 'best',\n",
    "                                            max_depth                 = self.max_depth,\n",
    "                                            min_samples_split         = self.min_samples_split,\n",
    "                                            min_samples_leaf          = self.min_samples_leaf,\n",
    "                                            min_weight_fraction_leaf  = self.min_weight_fraction_leaf,\n",
    "                                            min_impurity_decrease     = self.min_impurity_decrease,\n",
    "                                            min_impurity_split        = self.min_impurity_split,\n",
    "                                            max_features              = self.max_features,\n",
    "                                            max_leaf_nodes            = self.max_leaf_nodes,\n",
    "                                            random_state              = random_state,\n",
    "                                            presort                   = self.presort\n",
    "                                            )\n",
    "\n",
    "            if self.subsample < 1.0:\n",
    "                # no inplace multiplication!\n",
    "                sample_weight = sample_weight * sample_mask.astype(np.float64)\n",
    "                \n",
    "\n",
    "            tree.fit(X, residual, sample_weight=sample_weight)\n",
    "\n",
    "            # update tree leaves    \n",
    "            if self.Nestrov == True:\n",
    "                \n",
    "                y_pred                  = self.g_fitted_[i-1,k]\n",
    "                self.fitted_[i,k]       = loss.update_terminal_regions(tree.tree_, X, y_lower, y_higher, residual, y_pred,self.dist,self.sigma,sample_weight, sample_mask,self.learning_rate)\n",
    "                self.g_fitted_[i,k]     = (1-self.gamma[i-1])*self.fitted_[i,k]+self.gamma[i-1]*self.fitted_[i-1,k]\n",
    "                \n",
    "            else:\n",
    "                y_pred            = self.fitted_[i-1,k]\n",
    "                self.fitted_[i,k] = loss.update_terminal_regions(tree.tree_, X, y_lower,y_higher, residual, y_pred,self.dist,self.sigma,sample_weight, sample_mask,self.learning_rate)\n",
    "\n",
    "            # add tree to ensemble\n",
    "            self.estimators_[i, k] = tree\n",
    "    \n",
    "    def n_features(self):\n",
    "        return self.n_features_\n",
    "    \n",
    "    def _validate_y(self, y, sample_weight):\n",
    "        self.classes_    = np.unique(y)\n",
    "        self.n_classes_  = len(self.classes_)\n",
    "        return y\n",
    "    \n",
    "    def _fit_stages(self, X, y_lower, y_higher,sample_weight, random_state,\n",
    "                    X_val, y_lower_val, y_higher_val,sample_weight_val,begin_at_stage=0):\n",
    "        \n",
    "        \n",
    "        n_samples    = X.shape[0]\n",
    "        do_oob       = self.subsample < 1.0\n",
    "        sample_mask  = np.ones((n_samples, ), dtype=np.bool)\n",
    "        n_inbag      = max(1, int(self.subsample * n_samples))\n",
    "        loss_        = self.loss_\n",
    "        \n",
    "        # create one-hot label encoding\n",
    "        pred     = np.zeros((n_samples, self.loss_.K), dtype=np.float64)\n",
    "        pred_val = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "        \n",
    "        for k in range(self.loss_.K):\n",
    "            pred[:,k] = self.fitted_[0,k]\n",
    "            pred_val[:,k] = self.prev_valid_[0,k]\n",
    "            \n",
    "        if do_oob:\n",
    "            \n",
    "            sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n",
    "            self.train_score_[0] = loss_(y_lower[sample_mask],y_higher[sample_mask],pred[sample_mask],self.dist,self.sigma,self.metrics,sample_weight[sample_mask])\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            self.train_score_[0] = loss_(y_lower,y_higher,pred,self.dist,self.sigma,self.metrics,sample_weight)\n",
    "            \n",
    "        self.valid_score_[0] = loss_(y_lower_val,y_higher_val,pred_val,self.dist,self.sigma,self.metrics,sample_weight_val)\n",
    "\n",
    "        # perform boosting iterations\n",
    "        # validation loss performance\n",
    "        \n",
    "        for i in range(begin_at_stage, self.n_estimators):\n",
    "\n",
    "            # subsampling\n",
    "            if do_oob:\n",
    "                sample_mask = _random_sample_mask(n_samples, n_inbag, random_state)\n",
    "                \n",
    "            # fit next stage of trees\n",
    "            self._fit_stage(i, X, y_lower,y_higher, sample_weight,sample_mask, random_state)\n",
    "\n",
    "            if self.Nestrov == True:\n",
    "                \n",
    "                score = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "                \n",
    "                for k in range(self.loss_.K):\n",
    "                    score[:,k] = self.g_prev_valid_[i-1,k].copy()\n",
    "                    \n",
    "                predict_stage(self.estimators_, i, X_val, self.learning_rate, score)\n",
    "\n",
    "                for k in range(self.loss_.K):\n",
    "                    self.prev_valid_[i,k] = score[:,k].copy()\n",
    "                    \n",
    "                for k in range(self.loss_.K):\n",
    "                    self.g_prev_valid_[i,k] = (1-self.gamma[i-1])*self.prev_valid_[i,k]+self.gamma[i-1]*self.prev_valid_[i-1,k]\n",
    "            else:\n",
    "                \n",
    "                score = np.zeros((X_val.shape[0], self.loss_.K), dtype=np.float64)\n",
    "                for k in range(self.loss_.K):\n",
    "                    score[:,k] = self.prev_valid_[i-1,k].copy()\n",
    "\n",
    "                predict_stage(self.estimators_, i, X_val, self.learning_rate, score)\n",
    "                \n",
    "                for k in range(self.loss_.K):\n",
    "                    self.prev_valid_[i,k] = score[:,k].copy()\n",
    "\n",
    "            for k in range(self.loss_.K):\n",
    "                \n",
    "                pred[:,k] = self.fitted_[i,k]\n",
    "                pred_val[:,k] = self.prev_valid_[i,k]\n",
    "            \n",
    "            if do_oob:\n",
    "                self.train_score_[i] = loss_(y_lower[sample_mask],y_higher[sample_mask],pred[sample_mask],self.dist,self.sigma,self.metrics,sample_weight[sample_mask])\n",
    "            else:\n",
    "                self.train_score_[i] = loss_(y_lower,y_higher,pred,self.dist,self.sigma,self.metrics,sample_weight)\n",
    "   \n",
    "            self.valid_score_[i] = loss_(y_lower_val,y_higher_val,pred_val,self.dist,self.sigma,self.metrics,sample_weight_val)\n",
    "    \n",
    "        return i + 1\n",
    "\n",
    "    \n",
    "    def fit(self, X, y_lower,y_higher, sample_weight=None):\n",
    "        \n",
    "        # Check input\n",
    "        n_samples, self.n_features_ = X.shape\n",
    "        #y_lower                     = self._validate_y(y_lower, sample_weight)\n",
    "        #y_higher                    = self._validate_y(y_higher, sample_weight)\n",
    "        X                           = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        sample_weight               = np.ones(n_samples, dtype=np.float32)\n",
    "        \n",
    "        X, X_val, y_lower, y_lower_val,y_higher,y_higher_val,sample_weight, sample_weight_val \\\n",
    "        = train_test_split(X, y_lower,y_higher,sample_weight,random_state=self.random_state,test_size=self.validation_fraction)\n",
    "        self._check_params()\n",
    "        self._init_state()\n",
    "\n",
    "        # fit initial model - FIXME make sample_weight optional\n",
    "        #For Binomial       - init_ = LogOddsEstimator\n",
    "        #For Multinomial    - init_ = PriorProbabilityEstimator\n",
    "\n",
    "        self.init_.fit(X, y_lower,y_higher, X_val, y_lower_val,y_higher_val, sample_weight)\n",
    "        # init predictions and update in the inplace self\n",
    "        initial_pred,initial_val_pred  = self.init_.predict(X,X_val)\n",
    "        \n",
    "        for k in range(self.loss_.K):\n",
    "            self.fitted_[0,k], self.prev_valid_[0,k]          = initial_pred[:,k],initial_val_pred[:,k]\n",
    "            if self.Nestrov == True:\n",
    "                self.g_fitted_[0,k], self.g_prev_valid_[0,k]  = initial_pred[:,k],initial_val_pred[:,k]\n",
    "\n",
    "        begin_at_stage = 1\n",
    "        # fit the boosting stages\n",
    "        \n",
    "        n_stages = self._fit_stages(X, y_lower,y_higher, sample_weight, self.random_state,\n",
    "                                    X_val, y_lower_val,y_higher_val, sample_weight_val,begin_at_stage)\n",
    "        # change shape of arrays after fit (early-stopping or additional ests)\n",
    "        self.n_estimators_ = n_stages\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _make_estimator(self, append=True):\n",
    "        # we don't need _make_estimator\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def _init_decision_function(self, X):\n",
    "        \n",
    "        \"\"\"Check input and compute prediction of ``init``. \"\"\"\n",
    "        #self._check_initialized()\n",
    "        #X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)\n",
    "        if X.shape[1] != self.n_features_:\n",
    "            raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
    "                self.n_features_, X.shape[1]))\n",
    "        score = self.init_.predict(X).astype(np.float64)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def _decision_function(self, X):\n",
    "        \n",
    "        # for use in inner loop, not raveling the output in single-class case,\n",
    "        # not doing input validation.\n",
    "        \n",
    "        score = self._init_decision_function(X)\n",
    "        predict_stages(self.estimators_, X, self.learning_rate, score)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def _staged_decision_function(self, X):\n",
    "        \n",
    "        #X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        \n",
    "        score = self._init_decision_function(X)\n",
    "        for i in range(self.estimators_.shape[0]):\n",
    "            predict_stage(self.estimators_, i, X, self.learning_rate, score)\n",
    "            yield score.copy()\n",
    "    \n",
    "\n",
    "\n",
    "class GradientBoostingClassifier(BaseGradientBoosting):\n",
    "\n",
    "    _SUPPORTED_LOSS = ('survival')\n",
    "\n",
    "    def __init__(self, loss='aft', learning_rate=0.1, n_estimators=100,\n",
    "                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n",
    "                 min_samples_leaf=1, min_weight_fraction_leaf=0.,\n",
    "                 max_depth=3, min_impurity_decrease=0.,\n",
    "                 min_impurity_split=None, init=None,\n",
    "                 random_state=None, max_features=None, verbose=0,\n",
    "                 max_leaf_nodes=None, warm_start=False,\n",
    "                 presort='auto', validation_fraction=0.25,\n",
    "                 n_iter_no_change=None,Nestrov=False,metrics = 'logloss',dist='normal',sigma=1,tol=1e-4):\n",
    "\n",
    "        super().__init__(\n",
    "            loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n",
    "            criterion=criterion, min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "            max_depth=max_depth, init=init, subsample=subsample,\n",
    "            max_features=max_features,\n",
    "            random_state=random_state, verbose=verbose,\n",
    "            max_leaf_nodes=max_leaf_nodes,\n",
    "            min_impurity_decrease=min_impurity_decrease,\n",
    "            min_impurity_split=min_impurity_split,\n",
    "            warm_start=warm_start, presort=presort,\n",
    "            validation_fraction=validation_fraction,\n",
    "            n_iter_no_change=n_iter_no_change,Nestrov=Nestrov,metrics=metrics,\n",
    "            dist=dist,sigma=sigma,tol=tol)\n",
    "\n",
    "    def _validate_y(self, y, sample_weight):\n",
    "        #check_classification_targets(y)\n",
    "        self.classes_, y = np.unique(y, return_inverse=True)\n",
    "        n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n",
    "        if n_trim_classes < 2:\n",
    "            raise ValueError(\"y contains %d class after sample_weight \"\n",
    "                             \"trimmed classes with zero weights, while a \"\n",
    "                             \"minimum of 2 classes are required.\"\n",
    "                             % n_trim_classes)\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        return y\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n",
    "        score = self._decision_function(X)\n",
    "        if score.shape[1] == 1:\n",
    "            return score.ravel()\n",
    "        return score\n",
    "\n",
    "    #def staged_decision_function(self, X):\n",
    "    #    \n",
    "    #    yield from self._staged_decision_function(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "       \n",
    "        score     = self.decision_function(X)\n",
    "        decisions = self.loss_._score_to_decision(score)\n",
    "        return self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def staged_predict(self, X):\n",
    "       \n",
    "        for score in self._staged_decision_function(X):\n",
    "            decisions = self.loss_._score_to_decision(score)\n",
    "            yield self.classes_.take(decisions, axis=0)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        score = self.decision_function(X)\n",
    "        try:\n",
    "            return self.loss_._score_to_proba(score)\n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)\n",
    "\n",
    "    def predict_log_proba(self, X):\n",
    "        \n",
    "        proba = self.predict_proba(X)\n",
    "        return np.log(proba)\n",
    "\n",
    "    def staged_predict_proba(self, X):\n",
    "       \n",
    "        try:\n",
    "            for score in self._staged_decision_function(X):\n",
    "                yield self.loss_._score_to_proba(score)\n",
    "                \n",
    "        except NotFittedError:\n",
    "            raise\n",
    "        except AttributeError:\n",
    "            raise AttributeError('loss=%r does not support predict_proba' %\n",
    "                                 self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "def data_creation(file_name,K=5):\n",
    "    \n",
    "    df_train   = pd.read_csv(file_name)\n",
    "    y          = list(df_train['Y'])\n",
    "    req_cols   = [i for i in df_train.columns if i != 'Y']\n",
    "    X          = np.array(df_train[req_cols])\n",
    "    y_median   = np.percentile(y, 50) # return 50th percentile, e.g median.\n",
    "    bin_y      = list(map(lambda x : 0 if x < y_median else 1,y))\n",
    "\n",
    "    percentile = np.percentile(y, np.arange(0, 100, 100/K)) # deciles\n",
    "    multi_y    = list(map(lambda x : 0 if x >= percentile[0] and x< percentile[1] else 1 if x >= percentile[1] and x< percentile[2] else 2 if x >= percentile[2] and x< percentile[3] else 3,y))\n",
    "    \n",
    "    return X,y,bin_y,multi_y\n",
    "\n",
    "def chart_creation(gb,chart_title,chart_name):\n",
    "    \n",
    "    min_valid = round(np.min(gb.valid_score_),4)\n",
    "    min_train = round(np.min(gb.train_score_),4)\n",
    "    min_iter  = round(np.nanargmin(gb.valid_score_),0)\n",
    "\n",
    "    textstr = '\\n'.join((\n",
    "                    'Min Train = %.2f' % (min_train, ),\n",
    "                    'Min Valid = %.2f' % (min_valid, ),\n",
    "                    'Min Iter  = %.2f' % (min_iter, )))\n",
    "\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5,edgecolor=\"black\")\n",
    "    \n",
    "    fig,ax1       = plt.subplots()\n",
    "    ax2           = ax1.twinx()\n",
    "\n",
    "    ln1 = ax1.plot(gb.train_score_,color='blue',label='Training')\n",
    "    ln2 = ax2.plot(gb.valid_score_,color='orange',label='Validation')\n",
    "    \n",
    "    #ax1.axvline(x=np.nanargmin(gb.valid_score_),color='r')\n",
    "    #ax2.axhline(y=np.min(gb.valid_score_),color='b')\n",
    "    lns = ln1 + ln2\n",
    "    \n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc='lower left',fancybox='round', facecolor='wheat',fontsize=8)\n",
    "    \n",
    "    ax1.set_xlabel(\"Number of Iterations(Trees)\")\n",
    "    ax1.set_ylabel(\"Training Negative Likelihood(Loss)\")\n",
    "    ax2.set_ylabel(\"Validation Negative Likelihood(Loss)\")\n",
    "    #ax1.legend([\"Training\",\"Validation\"],loc='lower left',fancybox='round', facecolor='wheat',fontsize=8)\n",
    "    # place a text box in upper left in axes coords\n",
    "    ax2.text(0.7, 0.90, textstr, transform=ax1.transAxes, fontsize=8,\n",
    "        verticalalignment='top', bbox=props)\n",
    "    plt.title(chart_title)\n",
    "    plt.show()\n",
    "    fig.savefig(chart_name)\n",
    "    \n",
    "def generate_result(X,y_lower,y_higher,param):\n",
    "    \n",
    "    gb_manual = GradientBoostingClassifier(**param)\n",
    "    gb_manual.fit(X,y_lower,y_higher)\n",
    "    \n",
    "    return gb_manual    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data     = pd.read_csv('simulate_survival_uncen.csv')\n",
    "    X        = data[['x1','x2','x3']]\n",
    "    y_lower  = data['left']\n",
    "    y_higher = data['right']\n",
    "\n",
    "    param    = {'n_estimators' : 100,'learning_rate': 0.01,'Nestrov' : False,'subsample': 0.5,'min_samples_split': 10,\n",
    "                 'max_depth': 2,'metrics':'logloss','dist':'normal','sigma':2,'random_state' : 0}\n",
    "\n",
    "    gb_manual = generate_result(X,y_lower,y_higher,param)\n",
    "    chart_creation(gb_manual,'Nesterov=False,Loss=Logloss,Data=Uncensored','Nesterov_False_Loss_Logloss_Data_Uncensored.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
